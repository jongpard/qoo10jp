# -*- coding: utf-8 -*-
import os
import re
import csv
import json
import time
import math
import html
import uuid
import base64
import logging
import pathlib
import datetime as dt
from typing import List, Dict, Optional
from dataclasses import dataclass

import requests
from bs4 import BeautifulSoup

# Playwright (sync)
from playwright.sync_api import sync_playwright

# Google Drive (OAuth with refresh token)
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload

# ------------------------------
# ê¸°ë³¸ ì„¤ì •
# ------------------------------
URL = "https://www.qoo10.jp/gmkt.inc/Mobile/Bestsellers/Default.aspx?group_code=2"  # ë·°í‹° ì¹´í…Œê³ ë¦¬(ëª¨ë°”ì¼)
DATA_DIR = pathlib.Path("data")
DATA_DIR.mkdir(parents=True, exist_ok=True)

TODAY_KST = dt.datetime.utcnow() + dt.timedelta(hours=9)
DATE_STR = TODAY_KST.strftime("%Y-%m-%d")
CSV_NAME = f"íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_{DATE_STR}.csv"
CSV_PATH = DATA_DIR / CSV_NAME

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 13; Pixel 7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Mobile Safari/537.36",
    "Accept-Language": "ja,en;q=0.9,ko;q=0.8",
}

# Slack & Drive env
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL", "")
GDRIVE_FOLDER_ID = os.getenv("GDRIVE_FOLDER_ID", "")

GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID", "")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET", "")
GOOGLE_REFRESH_TOKEN = os.getenv("GOOGLE_REFRESH_TOKEN", "")

# ì˜µì…˜
TOP_N_FOR_SLACK = 10            # TOP10 ì¶œë ¥
MAX_DROP_FOR_SLACK = 5          # ê¸‰í•˜ë½ ìµœëŒ€ 5ê°œ
FORCE_PLAYWRIGHT = os.getenv("FORCE_PLAYWRIGHT", "").strip() == "1"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)

# ------------------------------
# ìœ í‹¸
# ------------------------------
def kst_now() -> dt.datetime:
    return dt.datetime.utcnow() + dt.timedelta(hours=9)

def yen_to_int(txt: str) -> Optional[int]:
    if not txt:
        return None
    txt = txt.replace(",", "").replace("Â¥", "").replace("å††", "").strip()
    m = re.search(r"(\d+)", txt)
    return int(m.group(1)) if m else None

def percent_to_int(txt: str) -> Optional[int]:
    if not txt:
        return None
    m = re.search(r"(\d+)", txt.replace("-", ""))
    return int(m.group(1)) if m else None

def clean_name(name: str) -> str:
    # Qoo10ì—ì„œ ë¶™ëŠ” "å…¬å¼" ë“± ì œê±°
    name = name.strip()
    name = re.sub(r"^\s*å…¬å¼\s*", "", name)
    # ê³¼í•œ ê´„í˜¸ ì œê±° ê·œì¹™(ì›í•˜ì‹œë©´ ì¡°ì •)
    name = re.sub(r"\s*ã€.*?ã€‘", "", name)
    name = re.sub(r"\s*\(.*?\)", "", name)
    return name.strip()

# ------------------------------
# ìˆ˜ì§‘ (HTTP / Playwright)
# ------------------------------
def fetch_http() -> str:
    logging.info("HTTP ìš”ì²­ ì‹œì‘: %s", URL)
    r = requests.get(URL, headers=HEADERS, timeout=30)
    r.raise_for_status()
    return r.text

def fetch_playwright() -> str:
    logging.info("Playwright ìˆ˜ì§‘ ì‹œì‘")
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        ctx = browser.new_context(
            user_agent=HEADERS["User-Agent"],
            locale="ja-JP",
            viewport={"width": 412, "height": 860},
        )
        page = ctx.new_page()
        page.goto(URL, wait_until="domcontentloaded", timeout=40_000)

        # ëª©ë¡ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° (ëª¨ë°”ì¼ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì˜ì—­)
        # ë‹¤ì–‘í•œ DOM ë³€í˜•ì— ëŒ€ì‘í•´ í›„ë³´ ì…€ë ‰í„° ì¤€ë¹„
        candidates = [
            "#lstBest li",            # ê¸°ì¡´ ëª¨ë°”ì¼ ë¦¬ìŠ¤íŠ¸
            ".best_list li",
            "ul li[id*='best']",
            "li .thumb"               # ìµœí›„ fallback
        ]
        found = False
        for sel in candidates:
            try:
                page.wait_for_selector(sel, timeout=8_000)
                found = True
                break
            except Exception:
                continue
        if not found:
            # ê·¸ë˜ë„ ì²«í™”ë©´ HTML ë°˜í™˜ (íŒŒì‹± ìª½ì—ì„œ 0ê±´ì²˜ë¦¬ ì‹œë„)
            logging.warning("Playwright: ì˜ˆìƒ ë¦¬ìŠ¤íŠ¸ ì…€ë ‰í„° íƒì§€ ì‹¤íŒ¨")
        html = page.content()
        ctx.close()
        browser.close()
        return html

# ------------------------------
# íŒŒì„œ
# ------------------------------
def parse_qoo10(html_text: str) -> List[Dict]:
    """
    ëª¨ë°”ì¼ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ëª©ë¡ íŒŒì‹±
    ë°˜í™˜: [{rank, brand, name, price, orig_price, discount_percent, url, product_code}]
    """
    soup = BeautifulSoup(html_text, "lxml")

    # ì£¼ìš” ë¦¬ìŠ¤íŠ¸ ì„ íƒì í›„ë³´
    containers = []
    for sel in ["#lstBest", ".best_list", "ul"]:
        nodes = soup.select(sel)
        if nodes:
            containers.extend(nodes)

    items: List[Dict] = []
    rank_seen = set()

    def extract_card(li) -> Optional[Dict]:
        # ë§í¬/ìƒí’ˆì½”ë“œ
        a = li.select_one("a[href*='/item/']")
        if not a:
            a = li.select_one("a[href]")
        if not a:
            return None
        href = a.get("href", "")
        if not href.startswith("http"):
            href = "https://www.qoo10.jp" + href
        m = re.search(r"/item/(\d+)", href)
        pid = m.group(1) if m else ""

        # íƒ€ì´í‹€
        title_el = li.select_one(".tit, .title, .goods_name, .prdName, .name")
        title = (title_el.get_text(" ", strip=True) if title_el else a.get_text(" ", strip=True)).strip()

        # ê°€ê²©
        price_el = li.select_one(".price, .prc, .num, .won, .sale")
        price = yen_to_int(price_el.get_text(" ", strip=True)) if price_el else None

        # ì •ê°€/í• ì¸
        orig_el = li.select_one(".org, .strike, .through")
        orig_price = yen_to_int(orig_el.get_text(" ", strip=True)) if orig_el else None

        disc_el = li.select_one(".per, .discount, .rate")
        discount_percent = percent_to_int(disc_el.get_text(" ", strip=True)) if disc_el else None

        # ë¸Œëœë“œ (ìˆìœ¼ë©´)
        brand_el = li.select_one(".brand, .shop, .mall")
        brand = brand_el.get_text(" ", strip=True) if brand_el else ""

        item = {
            "rank": None,  # í›„ì†ì—ì„œ ì±„ì›€
            "brand": brand,
            "name": clean_name(title),
            "price": price,
            "orig_price": orig_price,
            "discount_percent": discount_percent,
            "url": href,
            "product_code": pid,
        }
        return item

    # li ì¹´ë“œ íƒìƒ‰
    li_nodes = soup.select("#lstBest li") or soup.select(".best_list li") or soup.select("ul li")
    rank = 0
    for li in li_nodes:
        item = extract_card(li)
        if not item:
            continue
        rank += 1
        item["rank"] = rank
        if rank in rank_seen:
            continue
        rank_seen.add(rank)
        items.append(item)

    # rankê°€ ì•ˆë¶™ì—ˆë‹¤ë©´ fallback: ì²˜ìŒ 200ê°œê¹Œì§€ë§Œ ìˆœë²ˆ ë¶€ì—¬
    if items and items[0].get("rank") is None:
        for i, it in enumerate(items, start=1):
            it["rank"] = i

    # ì •ë¦¬
    items = [x for x in items if x.get("name")]
    return items

# ------------------------------
# ìˆ˜ì§‘ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (HTTP â†’ 0ê±´ì´ë©´ Playwright í´ë°±)
# ------------------------------
def fetch_products() -> List[Dict]:
    if FORCE_PLAYWRIGHT:
        logging.info("FORCE_PLAYWRIGHT=1 â†’ Playwright ê°•ì œ ì‚¬ìš©")
        html = fetch_playwright()
        items = parse_qoo10(html)
    else:
        # 1) HTTP
        try:
            html = fetch_http()
            items = parse_qoo10(html)
            if not items:
                logging.info("HTTP íŒŒì‹± 0ê±´ â†’ Playwright í´ë°± ì‹œë„")
                html = fetch_playwright()
                items = parse_qoo10(html)
        except Exception as e:
            logging.warning("HTTP ë‹¨ê³„ ì˜ˆì™¸ â†’ Playwright í´ë°±: %s", e)
            html = fetch_playwright()
            items = parse_qoo10(html)

    if not items:
        raise RuntimeError("Qoo10 ìˆ˜ì§‘ ê²°ê³¼ 0ê±´. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")

    return items

# ------------------------------
# CSV ì €ì¥ / ì½ê¸°
# ------------------------------
def save_csv(items: List[Dict], path: pathlib.Path) -> None:
    fields = ["date", "rank", "brand", "name", "price", "orig_price", "discount_percent", "url", "product_code"]
    with path.open("w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=fields)
        w.writeheader()
        for it in items:
            row = {
                "date": DATE_STR,
                "rank": it.get("rank"),
                "brand": it.get("brand", ""),
                "name": it.get("name", ""),
                "price": it.get("price"),
                "orig_price": it.get("orig_price"),
                "discount_percent": it.get("discount_percent"),
                "url": it.get("url"),
                "product_code": it.get("product_code"),
            }
            w.writerow(row)

def load_csv(path: pathlib.Path) -> List[Dict]:
    if not path.exists():
        return []
    rows = []
    with path.open("r", encoding="utf-8") as f:
        r = csv.DictReader(f)
        for row in r:
            # íƒ€ì… ë³´ì •
            for k in ("rank", "price", "orig_price", "discount_percent"):
                if row.get(k) not in (None, ""):
                    try:
                        row[k] = int(row[k])
                    except Exception:
                        row[k] = None
            rows.append(row)
    return rows

def yesterday_csv_path() -> Optional[pathlib.Path]:
    # ê°™ì€ í´ë” ë‚´ ì–´ì œ íŒŒì¼ì„ ì°¾ëŠ” ê°„ë‹¨í•œ ë°©ì‹
    y = (kst_now() - dt.timedelta(days=1)).strftime("%Y-%m-%d")
    p = DATA_DIR / f"íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_{y}.csv"
    return p if p.exists() else None

# ------------------------------
# ë­í‚¹ ë¹„êµ
# ------------------------------
@dataclass
class DiffItem:
    name: str
    brand: str
    prev_rank: Optional[int]
    curr_rank: Optional[int]
    url: str

def build_index(rows: List[Dict]) -> Dict[str, Dict]:
    """ë¹„êµ í‚¤: product_codeâ†’ ì—†ìœ¼ë©´ urlâ†’ ì—†ìœ¼ë©´ name"""
    idx = {}
    for r in rows:
        key = r.get("product_code") or r.get("url") or r.get("name")
        if key:
            idx[key] = r
    return idx

def compute_diffs(prev_rows: List[Dict], curr_rows: List[Dict]):
    prev_idx = build_index(prev_rows)
    curr_idx = build_index(curr_rows)

    # ê¸‰ìƒìŠ¹(ì „ì¼/ë‹¹ì¼ ëª¨ë‘ ì¡´ì¬, ìˆœìœ„ ê°œì„ )
    risers: List[DiffItem] = []
    # ê¸‰í•˜ë½(ì „ì¼/ë‹¹ì¼ ëª¨ë‘ ì¡´ì¬, ìˆœìœ„ í•˜ë½)
    fallers: List[DiffItem] = []
    # ë‰´ë­ì»¤(ì „ì¼ Top30 ë°–/ë¯¸ë“±ì¥ â†’ ë‹¹ì¼ Top30 ì§„ì…)
    newcomers: List[DiffItem] = []
    # ì¸&ì•„ì›ƒ ìˆ˜ (ì°¨íŠ¸ì¸ + ë­í¬ì•„ì›ƒ)
    inout_count = 0

    # ì „ì¼â†”ë‹¹ì¼ ë§¤ì¹­
    for key, cur in curr_idx.items():
        prev = prev_idx.get(key)
        if prev and prev.get("rank") and cur.get("rank"):
            delta = prev["rank"] - cur["rank"]
            if delta > 0:
                risers.append(DiffItem(
                    name=cur["name"], brand=cur.get("brand",""),
                    prev_rank=prev["rank"], curr_rank=cur["rank"], url=cur["url"]
                ))
            elif delta < 0:
                fallers.append(DiffItem(
                    name=cur["name"], brand=cur.get("brand",""),
                    prev_rank=prev["rank"], curr_rank=cur["rank"], url=cur["url"]
                ))

    # ë‰´ë­ì»¤: ì „ì¼ Top30 ë°–/ë¯¸ë“±ì¥ â†’ ì˜¤ëŠ˜ â‰¤30
    for key, cur in curr_idx.items():
        if cur.get("rank") and cur["rank"] <= 30:
            prev = prev_idx.get(key)
            if (not prev) or (prev.get("rank") is None) or prev["rank"] > 30:
                newcomers.append(DiffItem(
                    name=cur["name"], brand=cur.get("brand",""),
                    prev_rank=prev["rank"] if prev else None,
                    curr_rank=cur["rank"], url=cur["url"]
                ))

    # ì¸&ì•„ì›ƒ ê°œìˆ˜
    ins = len([d for d in newcomers if d.curr_rank and d.curr_rank <= 30])
    outs = 0
    for key, prv in prev_idx.items():
        if prv.get("rank") and prv["rank"] <= 30:
            cur = curr_idx.get(key)
            if (not cur) or (cur.get("rank") is None) or (cur["rank"] > 30):
                outs += 1
    inout_count = ins + outs

    # ì •ë ¬ ê·œì¹™
    risers.sort(key=lambda x: (x.prev_rank - x.curr_rank), reverse=True)  # ê°œì„ í­ desc
    fallers.sort(key=lambda x: (x.curr_rank - x.prev_rank), reverse=True)  # í•˜ë½í­ desc
    newcomers.sort(key=lambda x: x.curr_rank)

    return risers, newcomers, fallers, inout_count

# ------------------------------
# Slack
# ------------------------------
def slack_post(text: str):
    if not SLACK_WEBHOOK_URL:
        logging.info("[Slack] Webhook ë¯¸ì„¤ì • â†’ ìŠ¤í‚µ")
        return
    try:
        requests.post(SLACK_WEBHOOK_URL, json={"text": text}, timeout=10)
    except Exception as e:
        logging.warning("Slack ì „ì†¡ ì‹¤íŒ¨: %s", e)

def fmt_top10(rows: List[Dict]) -> str:
    lines = []
    for r in rows[:TOP_N_FOR_SLACK]:
        price = f"Â¥{r['price']:,}" if r.get("price") else ""
        disc = f" (â†“{r['discount_percent']}%)" if r.get("discount_percent") not in (None, "") else ""
        name = r["name"]
        brand = r.get("brand","")
        # ì œí’ˆëª… í•˜ì´í¼ë§í¬ (Slack ë§ˆí¬ë‹¤ìš´)
        link = f"<{r['url']}|{brand + ' ' if brand else ''}{name}>"
        lines.append(f"{r['rank']}. {link} â€” {price}{disc}")
    return "\n".join(lines)

def fmt_diff_line(d: DiffItem, updown: str) -> str:
    # - ì œí’ˆëª… 71ìœ„ â†’ 7ìœ„ (â†‘64)
    arrow = "â†‘" if updown == "up" else "â†“"
    return f"- {d.name} {d.prev_rank if d.prev_rank else 'OUT'}ìœ„ â†’ {d.curr_rank if d.curr_rank else 'OUT'}ìœ„ ({arrow}{abs((d.prev_rank or 0) - (d.curr_rank or 0))})"

def build_slack_message(curr_rows: List[Dict], prev_rows: List[Dict]) -> str:
    header = f"*Qoo10 ì¬íŒ¬ ë·°í‹° ë­í‚¹ â€” {DATE_STR}*"
    top10 = fmt_top10(curr_rows)

    risers, newcomers, fallers, inout_count = compute_diffs(prev_rows, curr_rows)

    if risers:
        risers_txt = "\n".join([fmt_diff_line(x, "up") for x in risers[:3]])
    else:
        risers_txt = "- í•´ë‹¹ ì—†ìŒ"

    if newcomers:
        newcomers_txt = "\n".join([fmt_diff_line(x, "up") for x in newcomers[:3]])
    else:
        newcomers_txt = "- í•´ë‹¹ ì—†ìŒ"

    if fallers:
        fallers_txt = "\n".join([fmt_diff_line(x, "down") for x in fallers[:MAX_DROP_FOR_SLACK]])
    else:
        fallers_txt = "- í•´ë‹¹ ì—†ìŒ"

    tail = f"{inout_count}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤."

    blocks = [
        header,
        "",
        "*TOP 10*",
        top10 or "- ë°ì´í„° ì—†ìŒ",
        "",
        "ğŸ”¥ *ê¸‰ìƒìŠ¹*",
        risers_txt,
        "",
        "ğŸ†• *ë‰´ë­ì»¤*",
        newcomers_txt,
        "",
        "ğŸ“‰ *ê¸‰í•˜ë½*",
        fallers_txt,
        "",
        f"ğŸ“¦ *ë­í¬ ì¸&ì•„ì›ƒ*",
        tail,
    ]
    return "\n".join(blocks)

# ------------------------------
# Google Drive ì—…ë¡œë“œ
# ------------------------------
def drive_upload(file_path: pathlib.Path) -> Optional[str]:
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN and GDRIVE_FOLDER_ID):
        logging.info("[Drive] env ë¯¸ì„¤ì • â†’ ìŠ¤í‚µ")
        return None
    try:
        creds = Credentials(
            None,
            refresh_token=GOOGLE_REFRESH_TOKEN,
            token_uri="https://oauth2.googleapis.com/token",
            client_id=GOOGLE_CLIENT_ID,
            client_secret=GOOGLE_CLIENT_SECRET,
        )
        service = build("drive", "v3", credentials=creds)
        media = MediaFileUpload(str(file_path), resumable=False)
        body = {"name": file_path.name, "parents": [GDRIVE_FOLDER_ID]}
        f = service.files().create(body=body, media_body=media, fields="id").execute()
        fid = f.get("id")
        logging.info("Google Drive ì—…ë¡œë“œ ì™„ë£Œ: %s", fid)
        return fid
    except Exception as e:
        logging.warning("Google Drive ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)
        return None

# ------------------------------
# main
# ------------------------------
def main():
    logging.info("ìˆ˜ì§‘ ì‹œì‘: %s", URL)
    t0 = time.time()

    items = fetch_products()
    logging.info("ìˆ˜ì§‘ ê°œìˆ˜: %d", len(items))

    # ì €ì¥
    save_csv(items, CSV_PATH)

    # ì „ì¼ CSV ë¡œë”©
    prev_path = yesterday_csv_path()
    prev_rows = load_csv(prev_path) if prev_path else []

    # Slack ë©”ì‹œì§€ êµ¬ì„±/ë°œì†¡
    msg = build_slack_message(items, prev_rows)
    slack_post(msg)

    # Google Drive ì—…ë¡œë“œ
    drive_upload(CSV_PATH)

    logging.info("ì´ ì†Œìš”: %.1fs", time.time() - t0)

if __name__ == "__main__":
    main()
