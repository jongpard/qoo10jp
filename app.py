# app.py
# Qoo10 Japan (ë·°í‹°) ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìˆ˜ì§‘ â†’ CSV ì €ì¥ â†’ ì „ì¼ ëŒ€ë¹„ ë³€í™” ì§‘ê³„ â†’ Slack ì „ì†¡ â†’ Google Drive ì—…ë¡œë“œ
# - /item/<ìˆ«ì> ë§í¬ë§Œ ìˆ˜ì§‘(ì¹´í…Œê³ ë¦¬/íƒ­/í”„ë¡œëª¨/ìë°”ìŠ¤í¬ë¦½íŠ¸ ë§í¬ í•„í„°ë§)
# - ìŠ¬ë™ ë©”ì‹œì§€: TOP 10 (í•˜ì´í¼ë§í¬), ğŸ”¥ ê¸‰ìƒìŠ¹/ ğŸ†• ë‰´ë­ì»¤/ ğŸ“‰ ê¸‰í•˜ë½(ê° 5ê°œ ì œí•œ), ë§í¬ ì¸&ì•„ì›ƒ ìš”ì•½
# - ì „ì¼ CSVê°€ ë¡œì»¬ì— ì—†ìœ¼ë©´, ë“œë¼ì´ë¸Œì—ì„œ ìµœì‹  ì „ì¼ íŒŒì¼ì„ í•œ ë²ˆ ì‹œë„í•´ ë‹¤ìš´ë¡œë“œ(ìˆìœ¼ë©´ ë¹„êµ)

from __future__ import annotations

import os
import re
import io
import csv
import sys
import time
import json
import shutil
import logging
from datetime import date, datetime
from typing import List, Dict, Optional, Tuple

import requests
from playwright.sync_api import sync_playwright

# Google Drive
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
from google.oauth2.credentials import Credentials

# ---------------------------
# ì„¤ì •
# ---------------------------
QOO10_BEAUTY_URL = "https://www.qoo10.jp/gmkt.inc/Mobile/Bestsellers/Default.aspx?group_code=2"
DATA_DIR = "data"
DEBUG_DIR = "data/debug"

MAX_RANK = int(os.getenv("QOO10_MAX_RANK", "200"))

SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL", "")
G_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID", "")
G_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET", "")
G_REFRESH_TOKEN = os.getenv("GOOGLE_REFRESH_TOKEN", "")
GDRIVE_FOLDER_ID = os.getenv("GDRIVE_FOLDER_ID", "")

# Slack í‘œì‹œ ì œí•œ
SHOW_TOP_N = 10
MAX_RISERS = 5
MAX_FALLERS = 5
MAX_NEW = 5

# ë¡œê¹…
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)

# ---------------------------
# ìœ í‹¸
# ---------------------------
def ensure_dirs():
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(DEBUG_DIR, exist_ok=True)

def yen(n: Optional[int]) -> str:
    if n is None:
        return "-"
    return f"Â¥{n:,}"

def percent_str(p: Optional[int]) -> str:
    if p is None:
        return ""
    return f" (â†“{p}%)"

def slack_link(url: str, text: str) -> str:
    # Slack ë§í¬ í¬ë§·: <url|text>
    safe = text.replace(">", "â€º").replace("|", "Â¦")
    return f"<{url}|{safe}>"

def today_str() -> str:
    return str(date.today())

def latest_prev_csv(prefix: str) -> Optional[str]:
    """
    data í´ë”ì—ì„œ prefixë¡œ ì‹œì‘í•˜ëŠ” ê°€ì¥ ìµœê·¼(ì˜¤ëŠ˜ ì´ì „) CSV ì°¾ê¸°
    íŒŒì¼ëª… ì˜ˆ: íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_2025-08-17.csv
    """
    if not os.path.isdir(DATA_DIR):
        return None
    files = [f for f in os.listdir(DATA_DIR) if f.startswith(prefix) and f.endswith(".csv")]
    if not files:
        return None
    # ë‚ ì§œ íŒŒì‹±
    cand: List[Tuple[str, datetime]] = []
    for f in files:
        m = re.search(r"(\d{4}-\d{2}-\d{2})", f)
        if not m:
            continue
        d = datetime.strptime(m.group(1), "%Y-%m-%d")
        if d.date() < date.today():
            cand.append((f, d))
    if not cand:
        return None
    cand.sort(key=lambda x: x[1], reverse=True)
    return os.path.join(DATA_DIR, cand[0][0])

def build_drive() -> Optional[any]:
    if not (G_CLIENT_ID and G_CLIENT_SECRET and G_REFRESH_TOKEN):
        return None
    creds = Credentials(
        token=None,
        refresh_token=G_REFRESH_TOKEN,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=G_CLIENT_ID,
        client_secret=G_CLIENT_SECRET,
        scopes=["https://www.googleapis.com/auth/drive.file", "https://www.googleapis.com/auth/drive.readonly"]
    )
    return build("drive", "v3", credentials=creds, cache_discovery=False)

def drive_upload_csv(drive, filepath: str, folder_id: str) -> Optional[str]:
    if drive is None or not os.path.isfile(filepath):
        return None
    fname = os.path.basename(filepath)
    file_metadata = {"name": fname, "parents": [folder_id]}
    media = MediaFileUpload(filepath, mimetype="text/csv", resumable=True)
    file = drive.files().create(body=file_metadata, media_body=media, fields="id").execute()
    return file.get("id")

def drive_download_latest_prev(drive, folder_id: str, prefix: str) -> Optional[str]:
    """
    ë“œë¼ì´ë¸Œ í´ë”ì—ì„œ prefix í¬í•¨ CSV ì¤‘, ì˜¤ëŠ˜ ì´ì „ ë‚ ì§œê°€ í¬í•¨ëœ ê°€ì¥ ìµœê·¼ íŒŒì¼ì„ ë°›ì•„ data/ ì— ì €ì¥
    """
    if drive is None:
        return None

    query = f"'{folder_id}' in parents and mimeType='text/csv' and name contains '{prefix}'"
    resp = drive.files().list(q=query, orderBy="createdTime desc", pageSize=50, fields="files(id,name)").execute()
    files = resp.get("files", [])
    for f in files:
        m = re.search(r"(\d{4}-\d{2}-\d{2})", f["name"])
        if not m:
            continue
        d = datetime.strptime(m.group(1), "%Y-%m-%d").date()
        if d < date.today():
            # download
            request = drive.files().get_media(fileId=f["id"])
            local = os.path.join(DATA_DIR, f["name"])
            with io.FileIO(local, "wb") as fh:
                downloader = MediaIoBaseDownload(fh, request)
                done = False
                while not done:
                    status, done = downloader.next_chunk()
            return local
    return None

# ---------------------------
# Qoo10 íŒŒì‹± í—¬í¼
# ---------------------------
def extract_price_yen(text: str) -> Optional[int]:
    """
    'Â¥3,200' / '3,200å††' ë“± ì—”í™” ê°€ê²© íŒŒì‹± (ì²« ë²ˆì§¸ í•­ëª©)
    """
    m = re.search(r'(?:Â¥\s*|)(\d{1,3}(?:,\d{3})+|\d+)\s*å††|Â¥\s*(\d{1,3}(?:,\d{3})+|\d+)', text)
    if not m:
        return None
    num = m.group(1) or m.group(2)
    return int(num.replace(',', ''))

def extract_discount_percent(text: str) -> Optional[int]:
    m = re.search(r'(\d{1,2}|100)\s*%[Oo][Ff][Ff]|â†“\s*(\d{1,2}|100)\s*%', text)
    if not m:
        return None
    v = m.group(1) or m.group(2)
    try:
        return int(v)
    except:
        return None

def clean_brand(s: str) -> str:
    s = s.strip()
    s = re.sub(r'^\s*å…¬å¼\s*', '', s)
    return s

def is_item_url(href: str) -> bool:
    if not href:
        return False
    if "javascript:" in href:
        return False
    if "/item/" not in href:
        return False
    # ì¹´í…Œê³ ë¦¬/ë”œ/ìŠ¤í˜ì…œ ë“±ì€ ì œì™¸
    if "/Mobile/Category" in href or "/Mobile/Deal" in href or "/Mobile/Special" in href:
        return False
    return True

def extract_product_id(href: str) -> Optional[str]:
    m = re.search(r'/item/(?:[^/]+/)?(\d+)', href)
    return m.group(1) if m else None

# ---------------------------
# ìˆ˜ì§‘ (í•µì‹¬)
# ---------------------------
def fetch_qoo10_beauty(max_count: int = MAX_RANK) -> List[Dict]:
    items: List[Dict] = []
    seen: set[str] = set()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        ctx = browser.new_context(user_agent=(
            "Mozilla/5.0 (Linux; Android 12; Pixel 5) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36"
        ))
        page = ctx.new_page()
        page.goto(QOO10_BEAUTY_URL, wait_until="domcontentloaded", timeout=60000)

        # ìƒí’ˆ ì•µì»¤ ëŒ€ê¸°
        page.wait_for_selector("a[href*='/item/']", timeout=30000)

        # lazy load í•´ì†Œë¥¼ ìœ„í•´ ìŠ¤í¬ë¡¤
        last_h = 0
        for _ in range(12):
            page.evaluate("window.scrollBy(0, document.body.scrollHeight)")
            time.sleep(0.6)
            h = page.evaluate("document.body.scrollHeight")
            if h == last_h:
                break
            last_h = h

        anchors = page.query_selector_all("a[href]")
        for a in anchors:
            href = (a.get_attribute("href") or "").strip()
            if not is_item_url(href):
                continue
            pid = extract_product_id(href)
            if not pid or pid in seen:
                continue

            li = a.closest("li") or a
            text = (li.inner_text() or "").strip()
            lines = [s.strip() for s in text.splitlines() if s.strip()]
            if not lines:
                continue

            # ì´ë¦„/ë¸Œëœë“œ íŒíŠ¸ ì¶”ì •
            name = lines[0]
            brand = ""
            if len(lines) >= 2 and len(lines[0]) <= 15 and len(lines[1]) >= 6:
                brand = clean_brand(lines[0])
                name = lines[1]

            price = extract_price_yen(text)
            if price is None:
                continue

            disc = extract_discount_percent(text)

            url = href if href.startswith("http") else ("https://www.qoo10.jp" + href)

            items.append({
                "date": today_str(),
                "rank": None,  # ë‚˜ì¤‘ì— ì±„ì›€
                "brand": brand,
                "name": name,
                "price": price,
                "orig_price": None,
                "discount_percent": disc,
                "url": url,
                "product_code": pid
            })
            seen.add(pid)
            if len(items) >= max_count:
                break

        ctx.close()
        browser.close()

    # ë­í¬ ì±„ìš°ê¸°
    for i, it in enumerate(items, start=1):
        it["rank"] = i

    return items

# ---------------------------
# CSV ì €ì¥/ë¡œë“œ
# ---------------------------
def csv_filename_for_today() -> str:
    return os.path.join(DATA_DIR, f"íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_{today_str()}.csv")

CSV_HEADERS = ["date", "rank", "brand", "name", "price", "orig_price", "discount_percent", "url", "product_code"]

def save_csv(path: str, rows: List[Dict]):
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=CSV_HEADERS)
        w.writeheader()
        for r in rows:
            row = {k: r.get(k, "") for k in CSV_HEADERS}
            w.writerow(row)

def load_csv(path: str) -> List[Dict]:
    out: List[Dict] = []
    with open(path, "r", encoding="utf-8") as f:
        for i, row in enumerate(csv.DictReader(f)):
            try:
                out.append({
                    "date": row.get("date", ""),
                    "rank": int(row.get("rank", "0") or 0),
                    "brand": row.get("brand", ""),
                    "name": row.get("name", ""),
                    "price": int(row.get("price", "0") or 0),
                    "orig_price": (int(row["orig_price"]) if row.get("orig_price") else None),
                    "discount_percent": (int(row["discount_percent"]) if row.get("discount_percent") else None),
                    "url": row.get("url", ""),
                    "product_code": row.get("product_code", ""),
                })
            except Exception as e:
                logging.warning("CSV load skip line %d: %s", i+2, e)
    return out

# ---------------------------
# ì „ì¼ ë¹„êµ(ê¸‰ìƒìŠ¹/ë‰´ë­ì»¤/ê¸‰í•˜ë½)
# ---------------------------
def compare_previous(curr: List[Dict], prev: List[Dict]) -> Tuple[List[Dict], List[Dict], List[Dict], int]:
    # product_codeë¥¼ í‚¤ë¡œ ë­í¬ ë¹„êµ
    prev_by_code = {r["product_code"]: r for r in prev if r.get("product_code")}
    curr_by_code = {r["product_code"]: r for r in curr if r.get("product_code")}

    # ìƒìŠ¹í­ = prev_rank - curr_rank (ê°’ì´ í´ìˆ˜ë¡ ê¸‰ìƒìŠ¹)
    movers = []
    for c in curr:
        code = c["product_code"]
        p = prev_by_code.get(code)
        if not p:
            continue
        delta = p["rank"] - c["rank"]
        if delta > 0:
            movers.append((delta, c))
    movers.sort(key=lambda x: x[0], reverse=True)
    risers = [m[1] for m in movers[:MAX_RISERS]]

    # ë‰´ë­ì»¤: prevì—ëŠ” ì—†ê³  currì—ë§Œ ìˆëŠ” í•­ëª©
    newcomers = [c for c in curr if c["product_code"] not in prev_by_code][:MAX_NEW]

    # ê¸‰í•˜ë½: prevì—ëŠ” ìˆì—ˆê³  currì—ë„ ìˆëŠ”ë° (curr_rank - prev_rank) ì–‘ìˆ˜ í° ìˆœ
    fallers_all = []
    for pcode, p in prev_by_code.items():
        c = curr_by_code.get(pcode)
        if not c:
            continue
        drop = c["rank"] - p["rank"]
        if drop > 0:
            fallers_all.append((drop, c))
    fallers_all.sort(key=lambda x: x[0], reverse=True)
    fallers = [f[1] for f in fallers_all[:MAX_FALLERS]]

    # ì¸&ì•„ì›ƒ: out ê°œìˆ˜(ì „ì¼ ì¡´ì¬, ê¸ˆì¼ ì—†ìŒ)
    out_count = sum(1 for pcode in prev_by_code if pcode not in curr_by_code)

    return risers, newcomers, fallers, out_count

# ---------------------------
# Slack ë©”ì‹œì§€
# ---------------------------
def build_slack_message(curr: List[Dict], risers: List[Dict], newcomers: List[Dict], fallers: List[Dict], out_count: int) -> str:
    title = f"*íí… ì¬íŒ¬ ë·°í‹° ë­í‚¹ â€” {today_str()}*"
    lines: List[str] = [title, "", "*TOP 10*"]

    for r in curr[:SHOW_TOP_N]:
        name = f"{(r['brand'] + ' ') if r['brand'] else ''}{r['name']}"
        price_part = f"{yen(r['price'])}{percent_str(r['discount_percent'])}"
        line = f"{r['rank']}. {slack_link(r['url'], name)} â€” {price_part}"
        lines.append(line)

    # ì„¹ì…˜ë“¤
    def section(title_emoji: str, rows: List[Dict]):
        lines.append("")
        lines.append(f"{title_emoji}")
        if not rows:
            lines.append("- í•´ë‹¹ ì—†ìŒ")
            return
        for r in rows:
            name = f"{(r['brand'] + ' ') if r['brand'] else ''}{r['name']}"
            lines.append(f"- {slack_link(r['url'], name)}")

    section("ğŸ”¥ ê¸‰ìƒìŠ¹", risers)
    section("ğŸ†• ë‰´ë­ì»¤", newcomers)
    section("ğŸ“‰ ê¸‰í•˜ë½", fallers)

    lines.append("")
    lines.append("ğŸ”— *ë§í¬ ì¸&ì•„ì›ƒ*")
    lines.append(f"{out_count}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return "\n".join(lines)

def post_slack(text: str):
    if not SLACK_WEBHOOK_URL:
        logging.info("[Slack] ì›¹í›… ë¯¸ì„¤ì •. ë©”ì‹œì§€ ì¶œë ¥ë§Œ í•©ë‹ˆë‹¤.")
        print(text)
        return
    resp = requests.post(SLACK_WEBHOOK_URL, data=json.dumps({"text": text}), headers={"Content-Type": "application/json"})
    if resp.status_code >= 300:
        logging.warning("[Slack] ì „ì†¡ ì‹¤íŒ¨: %s %s", resp.status_code, resp.text)

# ---------------------------
# ë©”ì¸
# ---------------------------
def main():
    ensure_dirs()
    logging.info("ìˆ˜ì§‘ ì‹œì‘: %s", QOO10_BEAUTY_URL)

    # ìˆ˜ì§‘
    items = fetch_qoo10_beauty(MAX_RANK)
    if not items:
        raise RuntimeError("Qoo10 ìˆ˜ì§‘ ê²°ê³¼ 0ê±´. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")

    logging.info("ìˆ˜ì§‘ ì™„ë£Œ: %d", len(items))

    # CSV ì €ì¥
    csv_path = csv_filename_for_today()
    save_csv(csv_path, items)
    logging.info("CSV ì €ì¥: %s", csv_path)

    # ì „ì¼ CSV í™•ë³´(ë¡œì»¬ â†’ ì—†ì„ ì‹œ ë“œë¼ì´ë¸Œì—ì„œ ì‹œë„)
    prefix = "íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_"
    prev_path = latest_prev_csv(prefix)

    drive = None
    if not prev_path and GDRIVE_FOLDER_ID:
        drive = build_drive()
        try:
            prev_path = drive_download_latest_prev(drive, GDRIVE_FOLDER_ID, prefix)
            if prev_path:
                logging.info("ë“œë¼ì´ë¸Œì—ì„œ ì „ì¼ CSV ë‹¤ìš´ë¡œë“œ: %s", prev_path)
        except Exception as e:
            logging.warning("ë“œë¼ì´ë¸Œ ì „ì¼ CSV ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: %s", e)

    # ë¹„êµ
    risers: List[Dict] = []
    newcomers: List[Dict] = []
    fallers: List[Dict] = []
    out_count = 0

    if prev_path and os.path.isfile(prev_path):
        prev_rows = load_csv(prev_path)
        risers, newcomers, fallers, out_count = compare_previous(items, prev_rows)
    else:
        logging.info("ì „ì¼ CSV ì—†ìŒ â†’ ë³€í™” ì„¹ì…˜ì€ ë¹„ì›€")

    # Slack ë©”ì‹œì§€
    msg = build_slack_message(items, risers, newcomers, fallers, out_count)
    post_slack(msg)
    logging.info("Slack ì „ì†¡ ì™„ë£Œ")

    # ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ
    if drive is None and GDRIVE_FOLDER_ID:
        drive = build_drive()
    if drive and GDRIVE_FOLDER_ID:
        try:
            file_id = drive_upload_csv(drive, csv_path, GDRIVE_FOLDER_ID)
            if file_id:
                logging.info("Google Drive ì—…ë¡œë“œ ì™„ë£Œ: %s", file_id)
        except Exception as e:
            logging.warning("Google Drive ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.exception("ì‹¤í–‰ ì‹¤íŒ¨: %s", e)
        sys.exit(1)
