# -*- coding: utf-8 -*-
"""
íí… ì¬íŒ¬ ë·°í‹°(https://www.qoo10.jp/gmkt.inc/Bestsellers/?g=2) ë­í‚¹ ìë™í™”
- 1ì°¨ ì†ŒìŠ¤: ëª¨ë°”ì¼ ë² ìŠ¤íŠ¸ì…€ëŸ¬ (ì •ì  HTML)
  https://www.qoo10.jp/gmkt.inc/Mobile/Bestsellers/Default.aspx?group_code=2
  (ë°±ì—…: ?__ar=Y íŒŒë¼ë¯¸í„° ë³€í˜•ë„ ì‹œë„)
- ì‹¤íŒ¨ ì‹œ: ë°ìŠ¤í¬í†± í˜ì´ì§€ë¥¼ Playwrightë¡œ í´ë°±
- íŒŒì¼ëª…: íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_YYYY-MM-DD.csv (KST)
- Top30 ë¹„êµ(ì „ì¼ CSV) ê¸°ì¤€ìœ¼ë¡œ Slack ë©”ì‹œì§€ ìƒì„± (TOP10 / ê¸‰ìƒìŠ¹ / ë‰´ë­ì»¤ / ê¸‰í•˜ë½(+OUT) / ì¸&ì•„ì›ƒ ê°œìˆ˜)

í™˜ê²½ë³€ìˆ˜:
  SLACK_WEBHOOK_URL
  GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, GOOGLE_REFRESH_TOKEN
  GDRIVE_FOLDER_ID
"""

import os, re, io, math, json, pytz, traceback
import datetime as dt
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple

import requests
import pandas as pd
from bs4 import BeautifulSoup

KST = pytz.timezone("Asia/Seoul")
MOBILE_URLS = [
    "https://www.qoo10.jp/gmkt.inc/Mobile/Bestsellers/Default.aspx?group_code=2",
    "https://www.qoo10.jp/gmkt.inc/Mobile/Bestsellers/Default.aspx?__ar=Y&group_code=2",
    "https://www.qoo10.jp/gmkt.inc/mobile/bestsellers/default.aspx?group_code=2",
]
DESKTOP_URL = "https://www.qoo10.jp/gmkt.inc/Bestsellers/?g=2"

# ---------- ì‹œê°„/ë¬¸ì ìœ í‹¸ ----------
def now_kst(): return dt.datetime.now(KST)
def today_kst_str(): return now_kst().strftime("%Y-%m-%d")
def yesterday_kst_str(): return (now_kst() - dt.timedelta(days=1)).strftime("%Y-%m-%d")
def build_filename(d): return f"íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_{d}.csv"
def clean_text(s): return re.sub(r"\s+", " ", (s or "")).strip()
def to_float(s):
    if s is None: return None
    try: return float(str(s).replace(",", ""))
    except: return None

# ---------- í†µí™”/í‘œê¸° ----------
YEN_NUM_RE = re.compile(r"(?<!\d)(\d{1,3}(?:,\d{3})+|\d+)(?![\d.])")
PCT_RE = re.compile(r"(\d+)\s*% ?OFF", re.I)

def parse_jpy_to_int(x: str) -> Optional[int]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì—” ê¸ˆì•¡ ì¶”ì¶œ â†’ int"""
    if not x: return None
    t = x.replace(",", "").replace("å††", "").strip()
    m = re.match(r"^\d+$", t)
    return int(t) if m else None

def fmt_currency_jpy(v) -> str:
    try:
        return f"Â¥{int(round(float(v))):,}"
    except:
        return "Â¥0"

def discount_floor(orig: Optional[float], sale: Optional[float], pct_txt: Optional[str]) -> Optional[int]:
    if pct_txt:
        m = re.search(r"\d+", pct_txt)
        if m: return int(m.group(0))
    if orig and sale and orig > 0:
        return max(0, int(math.floor((1 - sale / orig) * 100)))
    return None

def slack_escape(s): return s.replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

# ---------- ëª¨ë¸ ----------
@dataclass
class Product:
    rank: Optional[int]
    brand: str        # ì—†ìœ¼ë©´ ""
    title: str        # ìƒí’ˆëª…
    price: Optional[float]       # íŒë§¤ê°€
    orig_price: Optional[float]  # ì •ê°€(ì—†ìœ¼ë©´ None)
    discount_percent: Optional[int]
    url: str

def make_display_name(brand: str, product: str, include_brand: bool=True) -> str:
    product = clean_text(product); brand = clean_text(brand)
    if not include_brand or not brand: return product
    if re.match(rf"^\[?\s*{re.escape(brand)}\b", product, flags=re.I): return product
    return f"{brand} {product}"

# ---------- íŒŒì‹± ë¡œì§ (ëª¨ë°”ì¼ ì •ì ) ----------
def parse_mobile_html(html: str) -> List[Product]:
    """
    ëª¨ë°”ì¼ ë² ìŠ¤íŠ¸ì…€ëŸ¬ í˜ì´ì§€ëŠ” ì •ì  HTMLë¡œ ë‚´ë ¤ì™€ íŒŒì‹±ì´ ì‰¬ì›€.
    - ê° ì•„ì´í…œì€ a[href*='Goods.aspx']ë¥¼ í¬í•¨
    - í…ìŠ¤íŠ¸ ë¸”ë¡ ë‚´ì— 'xx%OFF orig sale' íŒ¨í„´ì´ ìì£¼ ë“±ì¥
    """
    soup = BeautifulSoup(html, "lxml")
    anchors = soup.select("a[href*='Goods.aspx']")
    items: List[Product] = []
    seen = set()
    for idx, a in enumerate(anchors, start=1):
        href = a.get("href", "")
        if not href: continue
        # ì¤‘ë³µ ì œê±°(ê°™ì€ ìƒí’ˆì´ ìƒ/í•˜ ì´ë¯¸ì§€ë¡œ 2ë²ˆ ë…¸ì¶œë˜ëŠ” ê²½ìš°)
        key = (href, clean_text(a.get_text(" ", strip=True)))
        if key in seen: continue
        seen.add(key)

        # ìƒìœ„ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        container = a.find_parent("li") or a.find_parent("div")
        block_text = clean_text(container.get_text(" ", strip=True)) if container else clean_text(a.get_text(" ", strip=True))
        name = clean_text(a.get_text(" ", strip=True))
        brand = ""  # íí…ì€ ë¸Œëœë“œ í•„ë“œê°€ ë¶„ë¦¬ë˜ì§€ ì•Šì€ ì¼€ì´ìŠ¤ å¤š

        # ê°€ê²©/í• ì¸ íŒŒì‹±
        pct = None; sale = None; orig = None
        m_pct = PCT_RE.search(block_text)
        if m_pct:
            pct = int(m_pct.group(1))
            tail = block_text[m_pct.end():]  # %OFF ë’¤ìª½ì—ì„œ ê°€ê²© 1~2ê°œ ë½‘ê¸°
            nums = [parse_jpy_to_int(m) for m in YEN_NUM_RE.findall(tail)]
            nums = [n for n in nums if n is not None]
            if len(nums) >= 2:
                orig, sale = nums[0], nums[1]
            elif len(nums) == 1:
                sale = nums[0]
        else:
            # %OFFê°€ ì—†ìœ¼ë©´ ë¸”ë¡ ì „ì²´ì—ì„œ ìˆ«ì í›„ë³´ ì¶”ì¶œ
            nums = [parse_jpy_to_int(m) for m in YEN_NUM_RE.findall(block_text)]
            nums = [n for n in nums if n is not None]
            # í›„ê¸°ê°œìˆ˜/í‰ì  ìˆ«ìë„ ì„ì—¬ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ, ë’¤ìª½ í° ê°’ 1~2ê°œë§Œ ì±„íƒ
            if nums:
                # í° ê°’ ìƒìœ„ 2ê°œ
                pick = sorted(nums)[-2:]
                if len(pick) == 2:
                    orig, sale = max(pick), min(pick)
                else:
                    sale = pick[-1]
        # í• ì¸ ì¬ê³„ì‚°(ë°±ì—…)
        pct = discount_floor(orig, sale, str(pct) if pct is not None else None)

        # ì ˆëŒ€ URL
        if href.startswith("//"): href = "https:" + href
        elif href.startswith("/"): href = "https://www.qoo10.jp" + href

        items.append(Product(rank=idx, brand=brand, title=name, price=sale, orig_price=orig, discount_percent=pct, url=href))
    # ìƒìœ„ 60ê°œê¹Œì§€ë§Œ (ë·°í‹° íƒ­ ì²« í™”ë©´ ê¸°ì¤€)
    return items[:60]

def fetch_by_http_mobile() -> List[Product]:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8,ko;q=0.7",
        "Cache-Control": "no-cache", "Pragma": "no-cache",
    }
    last_err = None
    for url in MOBILE_URLS:
        try:
            r = requests.get(url, headers=headers, timeout=20)
            r.raise_for_status()
            items = parse_mobile_html(r.text)
            if len(items) >= 10:
                print(f"[HTTP ëª¨ë°”ì¼] {url} â†’ {len(items)}ê°œ")
                return items
        except Exception as e:
            last_err = e
    if last_err: print("[HTTP ëª¨ë°”ì¼ ì˜¤ë¥˜]", last_err)
    return []

# ---------- Playwright (ë°ìŠ¤í¬í†± í´ë°±) ----------
def fetch_by_playwright() -> List[Product]:
    from playwright.sync_api import sync_playwright
    import time, pathlib

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=["--disable-blink-features=AutomationControlled","--no-sandbox","--disable-dev-shm-usage"],
        )
        context = browser.new_context(
            viewport={"width":1366,"height":900},
            locale="ja-JP",
            timezone_id="Asia/Tokyo",
            user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123 Safari/537.36"),
            extra_http_headers={"Accept-Language":"ja,en-US;q=0.9,en;q=0.8,ko;q=0.7"},
        )
        context.add_init_script("Object.defineProperty(navigator,'webdriver',{get:()=>undefined});")
        page = context.new_page()
        page.goto(DESKTOP_URL, wait_until="domcontentloaded", timeout=60_000)
        try: page.wait_for_load_state("networkidle", timeout=25_000)
        except: pass

        # ìŠ¤í¬ë¡¤ í´ë§
        start = dt.datetime.now().timestamp()
        found = 0
        while dt.datetime.now().timestamp() - start < 35:
            try: page.mouse.wheel(0, 1200)
            except: pass
            # a[href*='Goods.aspx'] ìˆ˜
            try:
                found = page.eval_on_selector_all("a[href*='Goods.aspx']", "els => els.length")
            except:
                found = 0
            if found >= 30: break
            page.wait_for_timeout(600)

        # JSì—ì„œ í•„ìš”í•œ í…ìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘ (name, href, containerText)
        data = page.evaluate("""
            () => {
              const as = Array.from(document.querySelectorAll("a[href*='Goods.aspx']"));
              const uniq = new Map();
              for (const a of as) {
                const href = a.getAttribute('href') || '';
                const name = (a.textContent || '').replace(/\\s+/g,' ').trim();
                if (!href || !name) continue;
                const li = a.closest('li') || a.closest('div');
                const block = (li ? li.innerText : a.innerText || '').replace(/\\s+/g,' ').trim();
                const key = href + '|' + name;
                if (!uniq.has(key)) uniq.set(key, {href, name, block});
              }
              return Array.from(uniq.values()).slice(0, 80);
            }
        """)
        context.close(); browser.close()

    items: List[Product] = []
    for i, row in enumerate(data, start=1):
        href = row.get("href",""); name = clean_text(row.get("name",""))
        block_text = clean_text(row.get("block",""))
        if href.startswith("//"): href = "https:" + href
        elif href.startswith("/"): href = "https://www.qoo10.jp" + href

        # ê°€ê²©/í• ì¸
        pct = None; sale = None; orig = None
        m_pct = PCT_RE.search(block_text)
        if m_pct:
            pct = int(m_pct.group(1))
            tail = block_text[m_pct.end():]
            nums = [parse_jpy_to_int(m) for m in YEN_NUM_RE.findall(tail)]
            nums = [n for n in nums if n is not None]
            if len(nums) >= 2:
                orig, sale = nums[0], nums[1]
            elif len(nums) == 1:
                sale = nums[0]
        else:
            nums = [parse_jpy_to_int(m) for m in YEN_NUM_RE.findall(block_text)]
            nums = [n for n in nums if n is not None]
            if nums:
                pick = sorted(nums)[-2:]
                if len(pick) == 2:
                    orig, sale = max(pick), min(pick)
                else:
                    sale = pick[-1]
        pct = discount_floor(orig, sale, str(pct) if pct is not None else None)

        items.append(Product(rank=i, brand="", title=name, price=sale, orig_price=orig, discount_percent=pct, url=href))
    return items[:60]

def fetch_products() -> List[Product]:
    items = fetch_by_http_mobile()
    if len(items) >= 10:
        return items
    print("[Playwright í´ë°± ì§„ì…]")
    return fetch_by_playwright()

# ---------- Google Drive (êµ­ë‚´/ê¸€ë¡œë²Œê³¼ ë™ì¼) ----------
def normalize_folder_id(raw: str) -> str:
    if not raw: return ""
    s = raw.strip()
    m = re.search(r"/folders/([a-zA-Z0-9_-]{10,})", s) or re.search(r"[?&]id=([a-zA-Z0-9_-]{10,})", s)
    return (m.group(1) if m else s)

def build_drive_service():
    from googleapiclient.discovery import build
    from google.oauth2.credentials import Credentials
    cid  = os.getenv("GOOGLE_CLIENT_ID")
    csec = os.getenv("GOOGLE_CLIENT_SECRET")
    rtk  = os.getenv("GOOGLE_REFRESH_TOKEN")
    if not (cid and csec and rtk):
        raise RuntimeError("OAuth ìê²©ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. GOOGLE_CLIENT_ID/SECRET/REFRESH_TOKEN í™•ì¸")
    creds = Credentials(
        None,
        refresh_token=rtk,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=cid,
        client_secret=csec,
    )
    svc = build("drive", "v3", credentials=creds, cache_discovery=False)
    try:
        about = svc.about().get(fields="user(displayName,emailAddress)").execute()
        u = about.get("user", {})
        print(f"[Drive] user={u.get('displayName')} <{u.get('emailAddress')}>")
    except Exception as e:
        print("[Drive] whoami ì‹¤íŒ¨:", e)
    return svc

def drive_upload_csv(service, folder_id: str, name: str, df: pd.DataFrame) -> str:
    from googleapiclient.http import MediaIoBaseUpload
    q = f"name = '{name}' and '{folder_id}' in parents and trashed = false"
    res = service.files().list(q=q, fields="files(id,name)",
                               supportsAllDrives=True, includeItemsFromAllDrives=True).execute()
    file_id = res.get("files", [{}])[0].get("id") if res.get("files") else None
    buf = io.BytesIO(); df.to_csv(buf, index=False, encoding="utf-8-sig"); buf.seek(0)
    media = MediaIoBaseUpload(buf, mimetype="text/csv", resumable=False)
    if file_id:
        service.files().update(fileId=file_id, media_body=media, supportsAllDrives=True).execute()
        return file_id
    meta = {"name": name, "parents": [folder_id], "mimeType": "text/csv"}
    created = service.files().create(body=meta, media_body=media, fields="id",
                                     supportsAllDrives=True).execute()
    return created["id"]

def drive_download_csv(service, folder_id: str, name: str) -> Optional[pd.DataFrame]:
    from googleapiclient.http import MediaIoBaseDownload
    res = service.files().list(q=f"name = '{name}' and '{folder_id}' in parents and trashed = false",
                               fields="files(id,name)", supportsAllDrives=True,
                               includeItemsFromAllDrives=True).execute()
    files = res.get("files", [])
    if not files: return None
    fid = files[0]["id"]
    req = service.files().get_media(fileId=fid, supportsAllDrives=True)
    fh = io.BytesIO(); dl = MediaIoBaseDownload(fh, req); done=False
    while not done: _, done = dl.next_chunk()
    fh.seek(0); return pd.read_csv(fh)

# ---------- Slack ----------
def slack_post(text: str):
    url = os.getenv("SLACK_WEBHOOK_URL")
    if not url:
        print("[ê²½ê³ ] SLACK_WEBHOOK_URL ë¯¸ì„¤ì • â†’ ì½˜ì†” ì¶œë ¥\n", text); return
    r = requests.post(url, json={"text": text}, timeout=20)
    if r.status_code >= 300:
        print("[Slack ì‹¤íŒ¨]", r.status_code, r.text)

# ---------- DF/ë¹„êµ/ë©”ì‹œì§€ ----------
def to_dataframe(products: List[Product], date_str: str) -> pd.DataFrame:
    return pd.DataFrame([{
        "date": date_str,
        "rank": p.rank,
        "brand": p.brand,
        "product_name": p.title,
        "price": p.price,
        "orig_price": p.orig_price,
        "discount_percent": p.discount_percent,
        "url": p.url,
    } for p in products])

def line_move(name_link: str, prev_rank: Optional[int], curr_rank: Optional[int]) -> Tuple[str, int]:
    if prev_rank is None and curr_rank is not None: return f"- {name_link} NEW â†’ {curr_rank}ìœ„", 99999
    if curr_rank is None and prev_rank is not None: return f"- {name_link} {prev_rank}ìœ„ â†’ OUT", 99999
    if prev_rank is None or curr_rank is None:    return f"- {name_link}", 0
    delta = prev_rank - curr_rank
    if   delta > 0: return f"- {name_link} {prev_rank}ìœ„ â†’ {curr_rank}ìœ„ (â†‘{delta})", delta
    elif delta < 0: return f"- {name_link} {prev_rank}ìœ„ â†’ {curr_rank}ìœ„ (â†“{abs(delta)})", abs(delta)
    else:           return f"- {name_link} {prev_rank}ìœ„ â†’ {curr_rank}ìœ„ (ë³€ë™ì—†ìŒ)", 0

def build_sections(df_today: pd.DataFrame, df_prev: Optional[pd.DataFrame]) -> Dict[str, List[str]]:
    S = {"top10": [], "rising": [], "newcomers": [], "falling": [], "outs": [], "inout_count": 0}

    # TOP 10 (ë¸Œëœë“œ í¬í•¨â€”íí…ì€ ë¸Œëœë“œ ë¶„ë¦¬ê°€ ì•ˆë  ìˆ˜ ìˆì–´ product_name ê·¸ëŒ€ë¡œ ë…¸ì¶œ)
    top10 = df_today.dropna(subset=["rank"]).sort_values("rank").head(10)
    for _, r in top10.iterrows():
        disp = make_display_name(r.get("brand",""), r["product_name"], include_brand=True)
        name_link = f"<{r['url']}|{slack_escape(disp)}>"
        price_txt = fmt_currency_jpy(r["price"])
        dc = r.get("discount_percent")
        tail = f" (â†“{int(dc)}%)" if pd.notnull(dc) else ""
        S["top10"].append(f"{int(r['rank'])}. {name_link} â€” {price_txt}{tail}")

    if df_prev is None or not len(df_prev):
        return S

    df_t = df_today.copy(); df_t["key"] = df_t["url"]; df_t.set_index("key", inplace=True)
    df_p = df_prev.copy(); df_p["key"] = df_p["url"]; df_p.set_index("key", inplace=True)

    t30 = df_t[(df_t["rank"].notna()) & (df_t["rank"] <= 30)].copy()
    p30 = df_p[(df_p["rank"].notna()) & (df_p["rank"] <= 30)].copy()
    common = set(t30.index) & set(p30.index)
    new    = set(t30.index) - set(p30.index)
    out    = set(p30.index) - set(t30.index)

    def full_name_link(row):
        disp = make_display_name(row.get("brand",""), row.get("product_name",""), include_brand=True)
        return f"<{row['url']}|{slack_escape(disp)}>"

    # ğŸ”¥ ê¸‰ìƒìŠ¹ (ìƒìœ„ 3)
    rising = []
    for k in common:
        prev_rank = int(p30.loc[k,"rank"]); curr_rank = int(t30.loc[k,"rank"])
        imp = prev_rank - curr_rank
        if imp > 0:
            line,_ = line_move(full_name_link(t30.loc[k]), prev_rank, curr_rank)
            rising.append((imp, curr_rank, prev_rank, slack_escape(t30.loc[k].get("product_name","")), line))
    rising.sort(key=lambda x: (-x[0], x[1], x[2], x[3]))
    S["rising"] = [e[-1] for e in rising[:3]]

    # ğŸ†• ë‰´ë­ì»¤ (â‰¤3)
    newcomers = []
    for k in new:
        curr_rank = int(t30.loc[k,"rank"])
        newcomers.append((curr_rank, f"- {full_name_link(t30.loc[k])} NEW â†’ {curr_rank}ìœ„"))
    newcomers.sort(key=lambda x: x[0])
    S["newcomers"] = [line for _, line in newcomers[:3]]

    # ğŸ“‰ ê¸‰í•˜ë½ (ìƒìœ„ 5)
    falling = []
    for k in common:
        prev_rank = int(p30.loc[k,"rank"]); curr_rank = int(t30.loc[k,"rank"])
        drop = curr_rank - prev_rank
        if drop > 0:
            line,_ = line_move(full_name_link(t30.loc[k]), prev_rank, curr_rank)
            falling.append((drop, curr_rank, prev_rank, slack_escape(t30.loc[k].get("product_name","")), line))
    falling.sort(key=lambda x: (-x[0], x[1], x[2], x[3]))
    S["falling"] = [e[-1] for e in falling[:5]]

    # OUT (ì „ì¼ Top30 â†’ ê¸ˆì¼ Top30 ë°–)
    for k in sorted(list(out)):
        prev_rank = int(p30.loc[k,"rank"])
        line,_ = line_move(full_name_link(p30.loc[k]), prev_rank, None)
        S["outs"].append(line)

    S["inout_count"] = len(new) + len(out)
    return S

def build_slack_message(date_str: str, S: Dict[str, List[str]]) -> str:
    lines: List[str] = []
    lines.append(f"*íí… ì¬íŒ¬ ë·°í‹° ë­í‚¹ â€” {date_str}*")
    lines.append("")
    lines.append("*TOP 10*");          lines.extend(S.get("top10") or ["- ë°ì´í„° ì—†ìŒ"]); lines.append("")
    lines.append("*ğŸ”¥ ê¸‰ìƒìŠ¹*");       lines.extend(S.get("rising") or ["- í•´ë‹¹ ì—†ìŒ"]); lines.append("")
    lines.append("*ğŸ†• ë‰´ë­ì»¤*");       lines.extend(S.get("newcomers") or ["- í•´ë‹¹ ì—†ìŒ"]); lines.append("")
    lines.append("*ğŸ“‰ ê¸‰í•˜ë½*");       lines.extend(S.get("falling") or ["- í•´ë‹¹ ì—†ìŒ"])
    lines.extend(S.get("outs") or [])
    lines.append(""); lines.append("*ğŸ”„ ë­í¬ ì¸&ì•„ì›ƒ*")
    lines.append(f"{S.get('inout_count', 0)}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return "\n".join(lines)

# ---------- ë©”ì¸ ----------
def main():
    date_str = today_kst_str()
    ymd_yesterday = yesterday_kst_str()
    file_today = build_filename(date_str)
    file_yesterday = build_filename(ymd_yesterday)

    print("ìˆ˜ì§‘ ì‹œì‘:", MOBILE_URLS[0])
    items = fetch_by_http_mobile()
    if len(items) < 10:
        print("[Playwright í´ë°± ì§„ì…]")
        items = fetch_by_playwright()
    print("ìˆ˜ì§‘ ì™„ë£Œ:", len(items))
    if len(items) < 10:
        raise RuntimeError("ì œí’ˆ ì¹´ë“œê°€ ë„ˆë¬´ ì ê²Œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")

    df_today = to_dataframe(items, date_str)
    os.makedirs("data", exist_ok=True)
    df_today.to_csv(os.path.join("data", file_today), index=False, encoding="utf-8-sig")
    print("ë¡œì»¬ ì €ì¥:", file_today)

    # Google Drive ì—…ë¡œë“œ + ì „ì¼ CSV ë¡œë“œ
    df_prev = None
    folder = normalize_folder_id(os.getenv("GDRIVE_FOLDER_ID",""))
    if folder:
        try:
            svc = build_drive_service()
            drive_upload_csv(svc, folder, file_today, df_today)
            print("Google Drive ì—…ë¡œë“œ ì™„ë£Œ:", file_today)
            df_prev = drive_download_csv(svc, folder, file_yesterday)
            print("ì „ì¼ CSV", "ì„±ê³µ" if df_prev is not None else "ë¯¸ë°œê²¬")
        except Exception as e:
            print("Google Drive ì²˜ë¦¬ ì˜¤ë¥˜:", e)
            traceback.print_exc()
    else:
        print("[ê²½ê³ ] GDRIVE_FOLDER_ID ë¯¸ì„¤ì • â†’ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ/ì „ì¼ ë¹„êµ ìƒëµ")

    S = build_sections(df_today, df_prev)
    msg = build_slack_message(date_str, S)
    slack_post(msg)
    print("Slack ì „ì†¡ ì™„ë£Œ")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("[ì˜¤ë¥˜ ë°œìƒ]", e); traceback.print_exc()
        try:
            slack_post(f"*íí… ì¬íŒ¬ ë·°í‹° ë­í‚¹ ìë™í™” ì‹¤íŒ¨*\n```\n{e}\n```")
        except: pass
        raise
