# -*- coding: utf-8 -*-
import os
import re
import csv
import io
import time
import json
import math
import datetime as dt
from typing import List, Dict, Tuple, Optional

# ---------- Playwright ----------
from playwright.sync_api import sync_playwright

# ---------- Drive (OAuth) ----------
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload
from google.oauth2.credentials import Credentials

# ---------- Requests / Parser ----------
from bs4 import BeautifulSoup

# =========================
# Config
# =========================
QOO10_URL = "https://www.qoo10.jp/gmkt.inc/Mobile/Bestsellers/Default.aspx?group_code=2"  # Beauty/ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£
DATA_DIR = "data"
TOP_LIMIT_SAVE = 200   # ì €ì¥ ìƒí•œ
TOP_LIMIT_COMPARE = 30 # ë¹„êµ êµ¬ê°„ ìƒí•œ (ê¸‰ìƒìŠ¹/ë‰´ë­ì»¤/ê¸‰í•˜ë½ ì‚°ì •ìš©)

# Slack / Drive ENV
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL", "")
GDRIVE_FOLDER_ID   = os.getenv("GDRIVE_FOLDER_ID", "")

GOOGLE_CLIENT_ID     = os.getenv("GOOGLE_CLIENT_ID", "")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET", "")
GOOGLE_REFRESH_TOKEN = os.getenv("GOOGLE_REFRESH_TOKEN", "")

# =========================
# Utils
# =========================
def kst_today() -> dt.date:
    # GitHub runner ê¸°ì¤€ UTC â†’ KST(+9)
    return (dt.datetime.utcnow() + dt.timedelta(hours=9)).date()

def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def clean_name(raw: str) -> str:
    if not raw:
        return raw
    name = raw
    # å…¬å¼ ì œê±°
    name = re.sub(r"^\s*å…¬å¼\s*", "", name)
    # ã€...ã€‘ / [...] / (...) ì œê±° (ë‚´ìš© í¬í•¨ í†µì§¸ ì œê±°)
    name = re.sub(r"ã€.*?ã€‘", "", name)
    name = re.sub(r"\[.*?\]", "", name)
    name = re.sub(r"\(.*?\)", "", name)
    # ê³µë°± ì •ë¦¬
    name = re.sub(r"\s+", " ", name).strip()
    return name

def extract_price(text: str) -> Optional[int]:
    if not text:
        return None
    # Â¥1,234 ë˜ëŠ” 1,234å†† íŒ¨í„´ ìš°ì„ 
    m = re.search(r"[Â¥ï¿¥]\s?([0-9,]+)", text)
    if not m:
        m = re.search(r"([0-9,]+)\s*å††", text)
    if not m:
        return None
    return int(m.group(1).replace(",", ""))

def extract_code_from_url(url: str) -> Optional[str]:
    if not url:
        return None
    # /item/<digits>
    m = re.search(r"/item/(\d+)", url)
    if m:
        return m.group(1)
    # ?no=123 ë“± ì˜ˆë¹„
    m = re.search(r"[?&](?:no|goods_code|goodscode|gdno)=(\d+)", url, re.I)
    if m:
        return m.group(1)
    # ë§ˆì§€ë§‰ ìˆ«ì í† í°
    m = re.search(r"(\d+)(?:$|\?)", url)
    return m.group(1) if m else None

def drive_client():
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN):
        return None
    creds = Credentials(
        None,
        refresh_token=GOOGLE_REFRESH_TOKEN,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=GOOGLE_CLIENT_ID,
        client_secret=GOOGLE_CLIENT_SECRET,
        scopes=["https://www.googleapis.com/auth/drive.file", "https://www.googleapis.com/auth/drive.readonly"]
    )
    return build("drive", "v3", credentials=creds)

def drive_find_prev_csv(service, prefix: str, today_name: str) -> Optional[Tuple[str, str]]:
    # ì˜¤ëŠ˜ íŒŒì¼ ì œì™¸í•˜ê³ , prefixë¡œ ì‹œì‘í•˜ëŠ” ìµœì‹  CSV í•˜ë‚˜
    q = f"'{GDRIVE_FOLDER_ID}' in parents and name contains '{prefix}' and mimeType='text/csv'"
    files = service.files().list(q=q, orderBy="createdTime desc",
                                 fields="files(id,name,createdTime)").execute().get("files", [])
    for f in files:
        if f["name"] != today_name:
            return f["id"], f["name"]
    return None

def drive_download_csv(service, file_id: str) -> List[Dict]:
    request = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        status, done = downloader.next_chunk()
    fh.seek(0)
    text = fh.read().decode("utf-8", errors="ignore")
    rows = []
    reader = csv.DictReader(io.StringIO(text))
    for r in reader:
        rows.append(r)
    return rows

def drive_upload_csv(service, path: str, filename: str) -> Optional[str]:
    meta = {"name": filename, "parents": [GDRIVE_FOLDER_ID]}
    media = MediaFileUpload(path, mimetype="text/csv", resumable=True)
    try:
        f = service.files().create(body=meta, media_body=media, fields="id").execute()
        return f.get("id")
    except Exception as e:
        print("[Drive] ì—…ë¡œë“œ ì‹¤íŒ¨:", e)
        return None

# =========================
# Scraping (Playwright)
# =========================
def fetch_qoo10() -> List[Dict]:
    """
    ëª¨ë°”ì¼ ë² ìŠ¤íŠ¸ì…€ëŸ¬ í˜ì´ì§€(ë·°í‹°: group_code=2)ì—ì„œ 200ìœ„ê¹Œì§€ íŒŒì‹±
    ë°˜í™˜: [{rank, name, price, url, code}]
    """
    items: List[Dict] = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--disable-dev-shm-usage"])
        ua = "Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1"
        context = browser.new_context(
            user_agent=ua,
            locale="ja-JP",
            viewport={"width": 390, "height": 844},
        )
        page = context.new_page()
        print("ìˆ˜ì§‘ ì‹œì‘:", QOO10_URL)
        page.goto(QOO10_URL, wait_until="domcontentloaded", timeout=60_000)

        # ê°„í—ì  ì¿ í‚¤/íŒì—…
        for sel in ["button#onetrust-accept-btn-handler", "button[aria-label='close']", "button:has-text('åŒæ„')"]:
            try:
                if page.locator(sel).first.is_visible():
                    page.locator(sel).first.click()
                    time.sleep(0.5)
            except:
                pass

        # ì»¨í…Œì´ë„ˆ ë“±ì¥ ëŒ€ê¸°
        # ëª¨ë°”ì¼ ë­í‚¹ ëª©ë¡ì—ì„œ ìƒí’ˆ a[href*="/item/"] ì´ ì¶©ë¶„íˆ ë³´ì¼ ë•Œê¹Œì§€
        page.wait_for_timeout(1000)
        for _ in range(40):  # ì•½ê°„ ë‚´ë ¤ì£¼ë©° ë¡œë”© ì•ˆì •í™”
            page.evaluate("window.scrollBy(0, 1000)")
            page.wait_for_timeout(150)
        # ìµœì¢… íŒŒì‹±
        html = page.content()
        browser.close()

    soup = BeautifulSoup(html, "lxml")

    # a[href*="/item/"] ê°€ ìƒí’ˆ ìƒì„¸ë¡œ ì—°ê²°ë¨
    anchors = soup.select('a[href*="/item/"]')
    # ì¤‘ë³µ ì œê±° (ë™ì¼ ì¹´ë“œ ë‚´ ì¤‘ë³µ ì•µì»¤ ì œê±° ìœ„í•œ href uniq)
    seen = set()
    cards = []
    for a in anchors:
        href = a.get("href", "")
        if "/item/" not in href:
            continue
        if href in seen:
            continue
        # ì£¼ë³€ì— ê°€ê²©/ì´ë¦„ì´ ê°™ì´ ìˆëŠ”ì§€ í™•ì¸ ìœ„í•´ ì¹´ë“œ(div/li) ìƒìœ„ë¡œ
        seen.add(href)
        cards.append(a)

    def nearest_text(el, selectors: List[str]) -> str:
        # a íƒœê·¸ ì£¼ë³€/ë¶€ëª¨ì—ì„œ í…ìŠ¤íŠ¸ íƒìƒ‰ (ê°€ê²©/ìƒí’ˆëª…)
        # ìš°ì„  ë¶€ëª¨ 3ë‹¨ê³„ê¹Œì§€ ê²€ìƒ‰
        cur = el
        txt = ""
        for up in range(0, 3):
            parent = cur.parent if cur else None
            if not parent:
                break
            for sel in selectors:
                cand = parent.select_one(sel)
                if cand and cand.get_text(strip=True):
                    return cand.get_text(" ", strip=True)
            cur = parent
        # ëª» ì°¾ìœ¼ë©´ ì—˜ë¦¬ë¨¼íŠ¸ ìì²´ í…ìŠ¤íŠ¸
        return el.get_text(" ", strip=True)

    # ì´ë¦„/ê°€ê²© ì…€ë ‰í„° í›„ë³´ (ëª¨ë°”ì¼ì—ì„œ ìì£¼ ë³´ì„)
    name_selectors = [
        ".sbj", ".tit", ".prd_tit", ".name", "p", "div.title", "span.title"
    ]
    price_selectors = [
        ".prc", ".price", ".org", ".dq_price", ".won", "em", "strong", "b", "span"
    ]

    rank = 1
    for a in cards:
        url = a.get("href", "")
        code = extract_code_from_url(url)
        raw_name = nearest_text(a, name_selectors)
        name = clean_name(raw_name)
        raw_price = nearest_text(a, price_selectors)
        price = extract_price(raw_price)

        if not name or not price or not code:
            continue

        items.append({
            "rank": rank,
            "name": name,
            "price": price,
            "url": url,
            "code": code
        })
        rank += 1
        if rank > TOP_LIMIT_SAVE:
            break

    return items

# =========================
# Compare
# =========================
def compare(today: List[Dict], prev: List[Dict]):
    # code ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ
    prev_map = {r.get("code"): r for r in prev if r.get("code")}
    today_map = {r.get("code"): r for r in today if r.get("code")}

    rising = []
    falling = []
    newcomers = []
    outs = []

    # ê¸‰ìƒìŠ¹/ê¸‰í•˜ë½: ì–‘ìª½ ëª¨ë‘ì— ì¡´ì¬í•˜ëŠ” ëŒ€ìƒ ì¤‘ ìƒìœ„ TOP_LIMIT_COMPARE ë‚´ì—ì„œë§Œ ë¹„êµ
    for t in today:
        c = t["code"]
        if c in prev_map:
            pr = int(prev_map[c].get("rank", 9999))
            tr = int(t["rank"])
            if pr <= TOP_LIMIT_COMPARE and tr <= TOP_LIMIT_COMPARE:
                diff = pr - tr  # +ë©´ ìƒìŠ¹
                if diff > 0:
                    rising.append({
                        "name": t["name"], "curr": tr, "prev": pr, "diff": diff
                    })
                elif diff < 0:
                    falling.append({
                        "name": t["name"], "curr": tr, "prev": pr, "diff": -diff
                    })
        else:
            # ë‰´ë­ì»¤: ì „ì¼ Top30 ë°–(ë˜ëŠ” ë¯¸ë“±ì¥) â†’ ë‹¹ì¼ Top30 ì§„ì…
            if int(t["rank"]) <= TOP_LIMIT_COMPARE:
                newcomers.append({
                    "name": t["name"], "curr": int(t["rank"])
                })

    # ì•„ì›ƒ: ì „ì¼ Top30ì¸ë° ì˜¤ëŠ˜ Top30 ë°–
    for p in prev:
        c = p.get("code")
        if not c:
            continue
        pr = int(p.get("rank", 9999))
        if pr <= TOP_LIMIT_COMPARE and c not in today_map:
            outs.append({"name": p["name"], "prev": pr})

    # ì •ë ¬ ë° ì œí•œ
    rising.sort(key=lambda x: (-x["diff"], x["curr"], x["prev"], x["name"]))
    falling.sort(key=lambda x: (-x["diff"], x["prev"], x["curr"], x["name"]))
    newcomers.sort(key=lambda x: (x["curr"], x["name"]))

    rising = rising[:3]
    newcomers = newcomers[:3]
    falling = falling[:5]

    inout_cnt = len(newcomers) + len(outs)
    return rising, newcomers, falling, inout_cnt

# =========================
# Slack
# =========================
def post_slack(payload: Dict):
    if not SLACK_WEBHOOK_URL:
        print("[Slack] WEBHOOK ë¯¸ì„¤ì •, ì¶œë ¥ë§Œ")
        print(json.dumps(payload, ensure_ascii=False, indent=2))
        return
    import requests
    r = requests.post(SLACK_WEBHOOK_URL, json=payload, timeout=20)
    print("[Slack] status:", r.status_code)

def slack_blocks(title: str, today: List[Dict], rising, newcomers, falling, inout_cnt) -> Dict:
    def top10_lines():
        lines = []
        for i, r in enumerate(today[:10], 1):
            price = f"Â¥{format(r['price'], ',')}"
            # Slack ë§í¬: <url|text>
            line = f"{i}. <{r['url']}|{r['name']}> â€” {price}"
            lines.append(line)
        return "\n".join(lines) if lines else "- í•´ë‹¹ ì—†ìŒ"

    def section_lines(lst, kind):
        rows = []
        if kind == "rising":
            for x in lst:
                rows.append(f"- {x['name']} {x['prev']}ìœ„ â†’ {x['curr']}ìœ„ (â†‘{x['diff']})")
        elif kind == "new":
            for x in lst:
                rows.append(f"- {x['name']} NEW â†’ {x['curr']}ìœ„")
        elif kind == "fall":
            for x in lst:
                rows.append(f"- {x['name']} {x['prev']}ìœ„ â†’ {x['curr']}ìœ„ (â†“{x['diff']})")
        return "\n".join(rows) if rows else "- í•´ë‹¹ ì—†ìŒ"

    blocks = [
        {"type": "section", "text": {"type": "mrkdwn", "text": f"*{title}*"}},
        {"type": "divider"},
        {"type": "section", "text": {"type": "mrkdwn", "text": "*TOP 10*"}},
        {"type": "section", "text": {"type": "mrkdwn", "text": top10_lines()}},
        {"type": "section", "text": {"type": "mrkdwn", "text": "*ğŸ”¥ ê¸‰ìƒìŠ¹*"}},
        {"type": "section", "text": {"type": "mrkdwn", "text": section_lines(rising, "rising")}},
        {"type": "section", "text": {"type": "mrkdwn", "text": "*ğŸ†• ë‰´ë­ì»¤*"}},
        {"type": "section", "text": {"type": "mrkdwn", "text": section_lines(newcomers, "new")}},
        {"type": "section", "text": {"type": "mrkdwn", "text": "*ğŸ“‰ ê¸‰í•˜ë½*"}},
        {"type": "section", "text": {"type": "mrkdwn", "text": section_lines(falling, "fall")}},
        {"type": "section", "text": {"type": "mrkdwn", "text": "*ğŸ“¦ ë­í¬ ì¸&ì•„ì›ƒ*"}},
        {"type": "section", "text": {"type": "mrkdwn", "text": f"{inout_cnt}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤."}},
    ]
    return {"blocks": blocks}

# =========================
# Main
# =========================
def main():
    ensure_dir(DATA_DIR)
    today = kst_today()
    ymd = today.strftime("%Y-%m-%d")
    title = f"íí… ì¬íŒ¬ ë·°í‹° ë­í‚¹ â€” {ymd}"

    # 1) ìˆ˜ì§‘
    products = fetch_qoo10()
    if not products:
        raise RuntimeError("Qoo10 ìˆ˜ì§‘ ê²°ê³¼ 0ê±´. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")

    # ìˆœìœ„ ë³´ì •(1..N)
    for i, r in enumerate(products, 1):
        r["rank"] = i

    # CSV ì €ì¥ (data/)
    csv_name = f"íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_{ymd}.csv"
    csv_path = os.path.join(DATA_DIR, csv_name)
    with open(csv_path, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["date", "rank", "name", "price", "url", "code"])
        for r in products:
            writer.writerow([ymd, r["rank"], r["name"], r["price"], r["url"], r["code"]])

    print("[ì €ì¥] CSV:", csv_path)

    # 2) Drive ì—…ë¡œë“œ + ì „ì¼ íŒŒì¼ ê°€ì ¸ì™€ ë¹„êµ
    rising = newcomers = falling = []
    inout_cnt = 0

    service = drive_client()
    if service and GDRIVE_FOLDER_ID:
        file_id = drive_upload_csv(service, csv_path, csv_name)
        if file_id:
            print("[Drive] ì—…ë¡œë“œ ì™„ë£Œ:", csv_name)
        else:
            print("[Drive] ì—…ë¡œë“œ ì‹¤íŒ¨")

        # ì „ì¼ íŒŒì¼(ìµœì‹  ì´ì „ë³¸) ê²€ìƒ‰
        prev = drive_find_prev_csv(service, "íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_", csv_name)
        prev_rows = []
        if prev:
            pid, pname = prev
            try:
                prev_rows = drive_download_csv(service, pid)
            except Exception as e:
                print("[Drive] ì´ì „ CSV ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨:", e)

        prev_items = []
        for r in prev_rows:
            try:
                prev_items.append({
                    "rank": int(r.get("rank", "9999")),
                    "name": r.get("name", ""),
                    "price": int(r.get("price", "0") or 0),
                    "url": r.get("url", ""),
                    "code": r.get("code", ""),
                })
            except:
                pass

        rising, newcomers, falling, inout_cnt = compare(products, prev_items)

    # 3) Slack ë©”ì‹œì§€ ì „ì†¡
    payload = slack_blocks(title, products, rising, newcomers, falling, inout_cnt)
    post_slack(payload)

if __name__ == "__main__":
    main()
