# app.py
# -*- coding: utf-8 -*-
"""
Qoo10 Japan Beauty Ranking Scraper
- ìˆ˜ì§‘: Qoo10 ëª¨ë°”ì¼ ë­í‚¹(ë·°í‹° ê·¸ë£¹) ê¸°ì¤€ ìµœëŒ€ 200ê°œ
- íŒŒì¼ëª…: íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_YYYY-MM-DD.csv (KST)
- Slack í¬ë§·: TOP10 â†’ ê¸‰ìƒìŠ¹(ìƒìœ„ 3) â†’ ë‰´ë­ì»¤(ìƒìœ„ 3) â†’ ê¸‰í•˜ë½(ìƒìœ„ 5) â†’ ë­í¬ ì¸&ì•„ì›ƒ(ê°œìˆ˜ë§Œ)
- ë¹„êµ ê¸°ì¤€: ì „ì¼ CSV (Drive í´ë” ë‚´ prefix ë§¤ì¹­, ê°€ì¥ ìµœê·¼ ë‚ ì§œ)
- í• ì¸ìœ¨: ì†Œìˆ˜ì  ì—†ì´ ë²„ë¦¼, ê´„í˜¸ í‘œê¸° (â†“27%)
- ì œí’ˆì½”ë“œ: URL ëì˜ ìˆ«ì id
"""

from __future__ import annotations

import os
import re
import csv
import time
import json
import math
import traceback
import datetime as dt
from pathlib import Path
from typing import List, Dict, Tuple, Optional

import requests
from bs4 import BeautifulSoup

# Playwright (ë™ì  ë Œë”ë§ í´ë°±)
from playwright.sync_api import sync_playwright

# Slack
import urllib.request
import urllib.error
import urllib.parse

# Google Drive (OAuth)
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.oauth2.credentials import Credentials


# =========================
# ê¸°ë³¸ ì„¤ì • / ê²½ë¡œ / ì‹œê°„ëŒ€
# =========================
KST = dt.timezone(dt.timedelta(hours=9))
TODAY = dt.datetime.now(KST).date()

BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# Qoo10 ëª¨ë°”ì¼ ë·°í‹° ë­í‚¹ URL
QOO10_URL = "https://www.qoo10.jp/gmkt.inc/Mobile/Bestsellers/Default.aspx?group_code=2"

# Slack ë²ˆì—­ ì˜µì…˜ (ì¼ë³¸ì–´ë§Œ â†’ í•œêµ­ì–´)
ENABLE_JA2KO = os.getenv("SLACK_TRANSLATE_JA2KO", "").strip() in ("1", "true", "TRUE")


def log(msg: str):
    print(msg, flush=True)


# =========================
# ìœ í‹¸: í…ìŠ¤íŠ¸/ìˆ«ì ì •ê·œí™”
# =========================
JP_BRACKET_PATTERNS = [
    r"ã€.*?ã€‘", r"ï¼».*?ï¼½", r"ã€”.*?ã€•", r"ã€ˆ.*?ã€‰", r"ã€Š.*?ã€‹", r"ã€Œ.*?ã€", r"ã€.*?ã€", r"\(.*?\)", r"ï¼ˆ.*?ï¼‰"
]

def normalize_text(s: str) -> str:
    if not s:
        return ""
    s = s.replace("\u3000", " ")  # ì „ê° ê³µë°±
    # "å…¬å¼" ì œê±°
    s = re.sub(r"^\s*å…¬å¼\s*", "", s).strip()
    # ê°ì¢… ê´„í˜¸ ë¸”ë¡ ì œê±°
    for pat in JP_BRACKET_PATTERNS:
        s = re.sub(pat, " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def parse_price(text: str) -> Optional[int]:
    if not text:
        return None
    m = re.findall(r"[\d,]+", text.replace(",", ""))
    if not m:
        return None
    try:
        return int(m[-1])
    except:
        return None

def extract_pid(url: str) -> str:
    # .../item/.../<PID> (ì¼ë°˜/ëª¨ë°”ì¼ ëª¨ë‘ ìˆ«ì idê°€ ëì— ì¡´ì¬)
    if not url:
        return ""
    m = re.search(r"/(\d+)(?:\?|$)", url)
    return m.group(1) if m else ""

def to_percent_floor(off: float) -> int:
    try:
        if off < 0:
            off = 0
        return math.floor(off)
    except:
        return 0

def has_japanese(text: str) -> bool:
    if not text:
        return False
    # íˆë¼ê°€ë‚˜, ê°€íƒ€ì¹´ë‚˜, ì¼ë¶€ í•œì ë²”ìœ„
    return re.search(r"[\u3040-\u30FF\u4E00-\u9FFF]", text) is not None


# =========================
# Slack
# =========================
def slack_post(text: str):
    url = os.environ.get("SLACK_WEBHOOK_URL", "")
    if not url:
        log("[Slack] SLACK_WEBHOOK_URL ë¯¸ì„¤ì •")
        return
    payload = json.dumps({"text": text}).encode("utf-8")
    req = urllib.request.Request(url, data=payload, headers={"Content-Type": "application/json"})
    try:
        with urllib.request.urlopen(req, timeout=10) as resp:
            log(f"[Slack] ì „ì†¡ ì™„ë£Œ: {resp.status}")
    except Exception as e:
        log(f"[Slack] ì „ì†¡ ì‹¤íŒ¨: {e}")


# =========================
# Translator (ì„ íƒ)
# =========================
def translate_ja_to_ko_batch(lines: List[str]) -> List[str]:
    if not ENABLE_JA2KO:
        return ["" for _ in lines]
    try:
        from googletrans import Translator  # ê°€ë²¼ìš´ ë²ˆì—­ê¸°(ì‹ ë¢°ì„±ì€ ë‚®ìœ¼ë‚˜ ë¬´ë£Œ)
        tr = Translator()
        outs = []
        for s in lines:
            if not s or not has_japanese(s):
                outs.append("")
                continue
            try:
                res = tr.translate(s, src="ja", dest="ko")
                outs.append(res.text)
            except Exception:
                outs.append("")
        return outs
    except Exception as e:
        log(f"[Translate] ì‚¬ìš© ì•ˆí•¨/ì˜¤ë¥˜: {e}")
        return ["" for _ in lines]


# =========================
# Google Drive
# =========================
def _drive_service():
    creds = Credentials(
        None,
        refresh_token=os.getenv("GOOGLE_REFRESH_TOKEN"),
        token_uri="https://oauth2.googleapis.com/token",
        client_id=os.getenv("GOOGLE_CLIENT_ID"),
        client_secret=os.getenv("GOOGLE_CLIENT_SECRET"),
        scopes=["https://www.googleapis.com/auth/drive.file"],
    )
    return build("drive", "v3", credentials=creds, cache_discovery=False)

def drive_upload_file(path: Path) -> Optional[str]:
    try:
        service = _drive_service()
        folder_id = os.getenv("GDRIVE_FOLDER_ID", "").strip()
        media = MediaFileUpload(str(path), mimetype="text/csv", resumable=True)
        file_metadata = {"name": path.name}
        if folder_id:
            file_metadata["parents"] = [folder_id]
        file = service.files().create(body=file_metadata, media_body=media, fields="id").execute()
        fid = file.get("id")
        log(f"[Drive] ì—…ë¡œë“œ ì™„ë£Œ: {fid}")
        return fid
    except Exception as e:
        log(f"[Drive] ì—…ë¡œë“œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
        return None

def drive_find_latest_prev(prefix: str) -> Optional[Path]:
    """Drive ì•ˆì—ì„œ prefixë¡œ ì‹œì‘í•˜ê³ , ì˜¤ëŠ˜ ë‚ ì§œ ì´ì „ íŒŒì¼ ì¤‘ ê°€ì¥ ìµœê·¼ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ."""
    try:
        service = _drive_service()
        folder_id = os.getenv("GDRIVE_FOLDER_ID", "").strip()
        q = f"name contains '{prefix}'"
        if folder_id:
            q = f"({q}) and '{folder_id}' in parents"

        results = service.files().list(
            q=q,
            fields="files(id, name, modifiedTime)",
            orderBy="modifiedTime desc",
            pageSize=50
        ).execute()
        files = results.get("files", [])

        target = None
        for f in files:
            name = f["name"]
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            m = re.search(r"(\d{4}-\d{2}-\d{2})", name)
            if not m:
                continue
            d = dt.datetime.strptime(m.group(1), "%Y-%m-%d").date()
            if d < TODAY:
                target = f
                break

        if not target:
            return None

        out = DATA_DIR / target["name"]
        with open(out, "wb") as fp:
            req = _drive_service().files().get_media(fileId=target["id"]).execute()
            fp.write(req)
        log(f"[Drive] ì „ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ: {out.name}")
        return out
    except Exception as e:
        log(f"[Drive] ì „ì¼ íƒìƒ‰/ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨(ë¬´ì‹œ): {e}")
        return None


# =========================
# CSV IO
# =========================
def save_csv(items: List[Dict], prefix: str) -> Path:
    path = DATA_DIR / f"{prefix}_{TODAY.isoformat()}.csv"
    with path.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["date", "rank", "brand", "product_name", "price", "url", "product_code"])
        for it in items:
            w.writerow([
                TODAY.isoformat(),
                it.get("rank"),
                it.get("brand", ""),
                it.get("name", ""),
                it.get("price", 0),
                it.get("url", ""),
                it.get("product_code", ""),
            ])
    log(f"[CSV] ì €ì¥: {path}")
    return path

def load_rows(csv_path: Optional[Path]) -> List[Dict]:
    if not csv_path or not csv_path.exists():
        return []
    rows = []
    with csv_path.open("r", encoding="utf-8") as f:
        r = csv.DictReader(f)
        for row in r:
            rows.append(row)
    return rows


# =========================
# ìˆ˜ì§‘ (HTTP â†’ ì‹¤íŒ¨ ì‹œ Playwright)
# =========================
def fetch_by_http(url: str, timeout: int = 15) -> List[Dict]:
    """Qoo10 ëª¨ë°”ì¼ ë­í‚¹(ë·°í‹°) HTML íŒŒì‹± (ì •ì ). í•„ìš” ì‹œ ë”ë³´ê¸°/ìŠ¤í¬ë¡¤ì€ Playwrightì—ì„œ ë‹´ë‹¹."""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }
    res = requests.get(url, headers=headers, timeout=timeout)
    if res.status_code != 200:
        return []
    soup = BeautifulSoup(res.text, "html.parser")
    return parse_qoo10_mobile_cards(soup)

def parse_qoo10_mobile_cards(soup: BeautifulSoup) -> List[Dict]:
    """ëª¨ë°”ì¼ í˜ì´ì§€ ì¹´ë“œ íŒŒì‹±. (ìƒìœ„ ì¼ë¶€ë§Œ ëœ° ìˆ˜ ìˆìŒ. ìµœëŒ€ 40~60ê°œ ì •ë„)"""
    items = []
    cards = soup.select("ul#best_prd_list li a[href*='/item/']")
    seen = set()
    rank = 1
    for a in cards:
        href = a.get("href") or ""
        if "/item/" not in href:
            continue
        pid = extract_pid(href)
        if not pid or pid in seen:
            continue
        seen.add(pid)

        name_el = a.select_one(".prd_txt, .name, .tit")
        brand_el = a.select_one(".brand, .brand_name")
        price_el = a.select_one(".price, .sale_price, .won.prc")

        name = normalize_text(name_el.get_text(strip=True) if name_el else "")
        brand = normalize_text(brand_el.get_text(strip=True) if brand_el else "")
        pr = parse_price(price_el.get_text(strip=True) if price_el else "")

        items.append({
            "rank": rank,
            "brand": brand,
            "name": name,
            "price": pr or 0,
            "url": urllib.parse.urljoin("https://www.qoo10.jp", href),
            "product_code": pid,
        })
        rank += 1
    return items

def fetch_by_playwright(url: str, target_count: int = 200) -> List[Dict]:
    """ëª¨ë°”ì¼ í˜ì´ì§€ì—ì„œ ìŠ¤í¬ë¡¤/ë”ë³´ê¸° ë“±ì„ í†µí•´ 200ê°œ ê·¼ì ‘ ìˆ˜ì§‘."""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--no-sandbox"])
        ctx = browser.new_context(viewport={"width": 390, "height": 844})
        page = ctx.new_page()
        page.goto(url, wait_until="networkidle", timeout=60_000)

        # ìµœëŒ€í•œ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ì¹´ë“œ ë¡œë“œ
        last_h = 0
        same_count = 0
        for _ in range(40):
            page.mouse.wheel(0, 4000)
            time.sleep(0.6)
            h = page.evaluate("document.body.scrollHeight")
            if h == last_h:
                same_count += 1
                if same_count >= 3:
                    break
            else:
                same_count = 0
            last_h = h

        html = page.content()
        browser.close()

    soup = BeautifulSoup(html, "html.parser")
    items = parse_qoo10_mobile_cards(soup)

    # í˜¹ì‹œ ë„ˆë¬´ ì ìœ¼ë©´ ë§í¬ ìˆ˜ì§‘ ë³´ê°• (ë‹¤ì–‘í•œ ì»¨í…Œì´ë„ˆ)
    if len(items) < target_count:
        extras = []
        links = soup.select("a[href*='/item/']")
        seen = {it["product_code"] for it in items}
        for a in links:
            href = a.get("href") or ""
            pid = extract_pid(href)
            if not pid or pid in seen:
                continue
            name = normalize_text(a.get_text(" ", strip=True))
            if len(name) < 2:
                continue
            price_el = a.select_one(".price, .sale_price, .won.prc")
            pr = parse_price(price_el.get_text(strip=True) if price_el else "")
            extras.append({
                "brand": "",
                "name": name,
                "price": pr or 0,
                "url": urllib.parse.urljoin("https://www.qoo10.jp", href),
                "product_code": pid,
            })
            seen.add(pid)
            if len(items) + len(extras) >= target_count:
                break
        # ìˆœìœ„ ë³´ì •
        rank = len(items) + 1
        for it in extras:
            it["rank"] = rank
            rank += 1
        items.extend(extras)

    # ìµœì¢… ìˆœìœ„ ì¬ì •ë ¬ (rank í•„ë“œ ê¸°ì¤€)
    items = sorted(items, key=lambda x: x["rank"])[:target_count]
    return items

def fetch_products() -> List[Dict]:
    log(f"ìˆ˜ì§‘ ì‹œì‘: {QOO10_URL}")
    items = fetch_by_http(QOO10_URL)
    if len(items) < 60:
        log("[HTTP] ê²°ê³¼ ì ìŒ â†’ Playwright í´ë°± ì§„ì…")
        items = fetch_by_playwright(QOO10_URL, target_count=200)

    log(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(items)}")
    if len(items) == 0:
        raise RuntimeError("Qoo10 ìˆ˜ì§‘ ê²°ê³¼ 0ê±´. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")
    return items


# =========================
# ë¹„êµ/ë¶„ì„
# =========================
def analyze(today_rows: List[Dict], prev_rows: List[Dict],
            top_new_threshold: int = 30,
            limit_rise: int = 3, limit_new: int = 3, limit_fall: int = 5) -> Tuple[List, List, List, int]:
    """
    - ê¸‰ìƒìŠ¹: prevì—ë„ ìˆê³  todayì—ë„ ìˆëŠ” ì œí’ˆ ì¤‘ rank ê°œì„ (prev - curr > 0) í° ìˆœ (tie: curr asc â†’ prev asc â†’ ì´ë¦„)
    - ë‰´ë­ì»¤: prev ì—†ê±°ë‚˜ prev_rank > threshold ì´ê³  today_rank <= threshold â†’ curr asc
    - ê¸‰í•˜ë½: prev/today ëª¨ë‘ ìˆê³  curr - prev > 0 â†’ ë‚´ë¦¼ì°¨ìˆœ, ìƒìœ„ 5
    - ì¸&ì•„ì›ƒ ìˆ˜: (ì¸ + ì•„ì›ƒ) ê°œìˆ˜
    """
    def key_name(d): return d.get("product_name","")

    def rows_to_map(rows):
        mp = {}
        for r in rows:
            code = r.get("product_code") or ""
            try:
                mp[code] = {
                    "rank": int(r.get("rank") or 9999),
                    "name": r.get("product_name",""),
                    "brand": r.get("brand",""),
                    "url": r.get("url",""),
                }
            except:
                pass
        return mp

    T = rows_to_map(today_rows)
    P = rows_to_map(prev_rows)

    rises = []
    new_rankers = []
    falls = []

    # ê¸‰ìƒìŠ¹/í•˜ë½ í›„ë³´
    for code, t in T.items():
        if code in P:
            curr = t["rank"]; prev = P[code]["rank"]
            if prev > curr:
                rises.append({
                    "name": t["name"], "curr": curr, "prev": prev, "delta": prev - curr
                })
            elif curr > prev:
                falls.append({
                    "name": t["name"], "curr": curr, "prev": prev, "delta": curr - prev
                })

    rises.sort(key=lambda x: (-x["delta"], x["curr"], x["prev"], x["name"]))
    falls.sort(key=lambda x: (-x["delta"], x["prev"], x["curr"], x["name"]))
    rises = rises[:limit_rise]
    falls = falls[:limit_fall]

    # ë‰´ë­ì»¤
    for code, t in T.items():
        curr = t["rank"]
        if curr <= top_new_threshold:
            if (code not in P) or (P[code]["rank"] > top_new_threshold):
                new_rankers.append({"name": t["name"], "curr": curr})
    new_rankers.sort(key=lambda x: x["curr"])
    new_rankers = new_rankers[:limit_new]

    # ì¸/ì•„ì›ƒ ì¹´ìš´íŠ¸
    ins = 0; outs = 0
    # in: prev>30 ë˜ëŠ” ë¯¸ë“±ì¥ â†’ today<=30
    for code, t in T.items():
        curr = t["rank"]
        if curr <= top_new_threshold:
            if (code not in P) or (P[code]["rank"] > top_new_threshold):
                ins += 1
    # out: prev<=30 â†’ today>30 ë˜ëŠ” ë¯¸ë“±ì¥
    for code, p in P.items():
        if p["rank"] <= top_new_threshold:
            if (code not in T) or (T[code]["rank"] > top_new_threshold):
                outs += 1

    return rises, new_rankers, falls, (ins + outs)


# =========================
# Slack í¬ë§· ë¹Œë”
# =========================
def line_top(rank: int, name: str, url: str, price: Optional[int], off_pct: Optional[int]) -> str:
    txt = f"{rank}. <{url}|{name}>"
    if price and price > 0:
        txt += f" â€” Â¥{price:,}"
    if off_pct is not None and off_pct > 0:
        txt += f" (â†“{off_pct}%)"
    return txt

def build_slack(today_rows: List[Dict], rises, new_rankers, falls, inout_cnt: int) -> str:
    title = f"*íí… ì¬íŒ¬ ë·°í‹° ë­í‚¹ â€” {TODAY.isoformat()}*"
    # TOP 10
    top10 = []
    for r in today_rows:
        try:
            rk = int(r["rank"])
        except:
            continue
        if rk > 10:
            continue
        name = r["product_name"]
        url = r["url"]
        price = int(r.get("price") or 0)

        # í• ì¸ìœ¨ì€ CSVì— ì—†ìœ¼ë‹ˆ (ëª¨ë°”ì¼ ì¹´ë“œì— í‘œì‹œê°€ ì¼ê´€ë˜ì§€ ì•Šì•„) 0ìœ¼ë¡œ í‘œì‹œ
        # í•„ìš” ì‹œ ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ orig_price/percentë¥¼ ë„£ì–´ í™•ì¥ ê°€ëŠ¥
        off = None

        top10.append(line_top(rk, name, url, price, off))

    # ë²ˆì—­(ì˜µì…˜): TOP10 ì´ë¦„ë§Œ
    trans_lines = []
    if ENABLE_JA2KO:
        src_names = [re.sub(r"^(\d+)\.\s+<[^|]+\|", "", ln).split(">")[0] for ln in top10]
        kos = translate_ja_to_ko_batch(src_names)
        for i, ko in enumerate(kos):
            if ko:
                top10[i] = f"{top10[i]}\n{ko}"

    # ê¸‰ìƒìŠ¹
    sec_rise = ["- í•´ë‹¹ ì—†ìŒ"] if not rises else [f"- {x['name']} {x['prev']}ìœ„ â†’ {x['curr']}ìœ„ (â†‘{x['delta']})" for x in rises]
    # ë‰´ë­ì»¤
    sec_new = ["- í•´ë‹¹ ì—†ìŒ"] if not new_rankers else [f"- {x['name']} NEW â†’ {x['curr']}ìœ„" for x in new_rankers]
    # ê¸‰í•˜ë½ (ìµœëŒ€ 5ê°œ)
    sec_fall = ["- í•´ë‹¹ ì—†ìŒ"] if not falls else [f"- {x['name']} {x['prev']}ìœ„ â†’ {x['curr']}ìœ„ (â†“{x['delta']})" for x in falls]

    # ì „ì²´ ë©”ì‹œì§€
    parts = [
        title,
        "",
        "*TOP 10*",
        *top10,
        "",
        "ğŸ”¥ *ê¸‰ìƒìŠ¹*",
        *sec_rise,
        "",
        "ğŸ†• *ë‰´ë­ì»¤*",
        *sec_new,
        "",
        "ğŸ“‰ *ê¸‰í•˜ë½*",
        *sec_fall,
        "",
        "ğŸ” *ë­í¬ ì¸&ì•„ì›ƒ*",
        f"{inout_cnt}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.",
    ]
    return "\n".join(parts)


# =========================
# íŒŒì´í”„ë¼ì¸
# =========================
def qoo10_pipeline(items: List[Dict]):
    log(f"[QOO10] collected items: {len(items)}")
    (DATA_DIR / "debug_items.json").write_text(json.dumps(items, ensure_ascii=False, indent=2), encoding="utf-8")

    if len(items) == 0:
        raise RuntimeError("QOO10 ìˆ˜ì§‘ ê²°ê³¼ê°€ 0ê±´ì…ë‹ˆë‹¤. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")

    # ì •ê·œí™” + PID
    for it in items:
        it["name"] = normalize_text(it.get("name",""))
        it["brand"] = normalize_text(it.get("brand",""))
        it["product_code"] = extract_pid(it.get("url",""))

    # CSV ì €ì¥
    csv_path = save_csv(items, prefix="íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹")

    # Drive ì—…ë¡œë“œ (ë¬´ì‹œ ê°€ëŠ¥)
    drive_upload_file(csv_path)

    # ì˜¤ëŠ˜/ì „ì¼ ë¡œë“œ
    today_rows = load_rows(csv_path)
    prev_path  = drive_find_latest_prev("íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_")
    prev_rows  = load_rows(prev_path) if prev_path else []

    rises, new_rankers, falls, inout_cnt = analyze(today_rows, prev_rows,
                                                   top_new_threshold=30,
                                                   limit_rise=3, limit_new=3, limit_fall=5)
    msg = build_slack(today_rows, rises, new_rankers, falls, inout_cnt)
    slack_post(msg)


# =========================
# main
# =========================
def main():
    try:
        items = fetch_products()
        qoo10_pipeline(items)
    except Exception as e:
        log("[ì—ëŸ¬] " + repr(e))
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main()
